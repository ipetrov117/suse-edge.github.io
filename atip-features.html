<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | Telco features configuration</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Telco features configuration"/>
<meta name="description" content="This section documents and explains the configuration of Telco-specific features on ATIP-deployed clusters."/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 35. Telco features configuration"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Telco features configuration"/>
<meta property="og:description" content="This section documents and explains the configuration of Te…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Telco features configuration"/>
<meta name="twitter:description" content="This section documents and explains the configuration of Te…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "Telco features configuration",
  
    "description": "Telco features configuration",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="atip-management-cluster.html" title="Chapter 34. Setting up the management cluster"/><link rel="next" href="atip-automated-provisioning.html" title="Chapter 36. Fully automated directed network provisioning"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-product-documentation.html">Product Documentation</a><span> / </span><a class="crumb" href="atip-features.html">Telco features configuration</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="id-suse-edge-3-2-0-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.2.0 Documentation</span></a></li><li><a href="id-quick-starts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">4 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">7 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">8 </span><span class="title-name">SLE Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">9 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">10 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">12 </span><span class="title-name">Elemental</span></a></li><li><a href="components-akri.html" class=" "><span class="title-number">13 </span><span class="title-name">Akri</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">14 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">15 </span><span class="title-name">RKE2</span></a></li><li><a href="components-longhorn.html" class=" "><span class="title-number">16 </span><span class="title-name">Longhorn</span></a></li><li><a href="components-neuvector.html" class=" "><span class="title-number">17 </span><span class="title-name">NeuVector</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">18 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">19 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">20 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">21 </span><span class="title-name">Upgrade Controller</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">22 </span><span class="title-name">MetalLB on K3s (using L2)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">23 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">24 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">25 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">26 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-sle-micro.html" class=" "><span class="title-number">27 </span><span class="title-name">NVIDIA GPUs on SLE Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">V </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">28 </span><span class="title-name">Edge 3.1 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">29 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">30 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li class="active"><a href="id-product-documentation.html" class="has-children you-are-here"><span class="title-number">VI </span><span class="title-name">Product Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">31 </span><span class="title-name">SUSE Adaptive Telco Infrastructure Platform (ATIP)</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">32 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">33 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">34 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" you-are-here"><span class="title-number">35 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" "><span class="title-number">36 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">37 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">VII </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">38 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip-features" data-id-title="Telco features configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">35 </span><span class="title-name">Telco features configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section documents and explains the configuration of Telco-specific features on ATIP-deployed clusters.</p><p>The directed network provisioning deployment method is used, as described in the ATIP Automated Provision (<a class="xref" href="atip-automated-provisioning.html" title="Chapter 36. Fully automated directed network provisioning">Chapter 36, <em>Fully automated directed network provisioning</em></a>) section.</p><p>The following topics are covered in this section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel image for real time (<a class="xref" href="atip-features.html#kernel-image-for-real-time" title="35.1. Kernel image for real time">Section 35.1, “Kernel image for real time”</a>): Kernel image to be used by the real-time kernel.</p></li><li class="listitem"><p>Kernel arguments for low latency and high performance (<a class="xref" href="atip-features.html#kernel-args" title="35.2. Kernel arguments for low latency and high performance">Section 35.2, “Kernel arguments for low latency and high performance”</a>): Kernel arguments to be used by the real-time kernel for maximum performance and low latency running telco workloads.</p></li><li class="listitem"><p>CPU tuned configuration (<a class="xref" href="atip-features.html#cpu-tuned-configuration" title="35.3. CPU tuned configuration">Section 35.3, “CPU tuned configuration”</a>): Tuned configuration to be used by the real-time kernel.</p></li><li class="listitem"><p>CNI configuration (<a class="xref" href="atip-features.html#cni-configuration" title="35.4. CNI Configuration">Section 35.4, “CNI Configuration”</a>): CNI configuration to be used by the Kubernetes cluster.</p></li><li class="listitem"><p>SR-IOV configuration (<a class="xref" href="atip-features.html#sriov" title="35.5. SR-IOV">Section 35.5, “SR-IOV”</a>): SR-IOV configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>DPDK configuration (<a class="xref" href="atip-features.html#dpdk" title="35.6. DPDK">Section 35.6, “DPDK”</a>): DPDK configuration to be used by the system.</p></li><li class="listitem"><p>vRAN acceleration card (<a class="xref" href="atip-features.html#acceleration" title="35.7. vRAN acceleration (Intel ACC100/ACC200)">Section 35.7, “vRAN acceleration (<code class="literal">Intel ACC100/ACC200</code>)”</a>): Acceleration card configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Huge pages (<a class="xref" href="atip-features.html#huge-pages" title="35.8. Huge pages">Section 35.8, “Huge pages”</a>): Huge pages configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>CPU pinning configuration (<a class="xref" href="atip-features.html#cpu-pinning-configuration" title="35.9. CPU pinning configuration">Section 35.9, “CPU pinning configuration”</a>): CPU pinning configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>NUMA-aware scheduling configuration (<a class="xref" href="atip-features.html#numa-aware-scheduling" title="35.10. NUMA-aware scheduling">Section 35.10, “NUMA-aware scheduling”</a>): NUMA-aware scheduling configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Metal LB configuration (<a class="xref" href="atip-features.html#metal-lb-configuration" title="35.11. Metal LB">Section 35.11, “Metal LB”</a>): Metal LB configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Private registry configuration (<a class="xref" href="atip-features.html#private-registry" title="35.12. Private registry configuration">Section 35.12, “Private registry configuration”</a>): Private registry configuration to be used by the Kubernetes workloads.</p></li></ul></div><section class="sect1" id="kernel-image-for-real-time" data-id-title="Kernel image for real time"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.1 </span><span class="title-name">Kernel image for real time</span></span> <a title="Permalink" class="permalink" href="atip-features.html#kernel-image-for-real-time">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The real-time kernel image is not necessarily better than a standard kernel.
It is a different kernel tuned to a specific use case. The real-time kernel is tuned for lower latency at the cost of throughput. The real-time kernel is not recommended for general purpose use, but in our case, this is the recommended kernel for Telco Workloads where latency is a key factor.</p><p>There are four top features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Deterministic execution:</p><p>Get greater predictability — ensure critical business processes complete in time, every time and deliver high-quality service, even under heavy system loads. By shielding key system resources for high-priority processes, you can ensure greater predictability for time-sensitive applications.</p></li><li class="listitem"><p>Low jitter:</p><p>The low jitter built upon the highly deterministic technology helps to keep applications synchronized with the real world. This helps services that need ongoing and repeated calculation.</p></li><li class="listitem"><p>Priority inheritance:</p><p>Priority inheritance refers to the ability of a lower priority process to assume a higher priority when there is a higher priority process that requires the lower priority process to finish before it can accomplish its task. SUSE Linux Enterprise Real Time solves these priority inversion problems for mission-critical processes.</p></li><li class="listitem"><p>Thread interrupts:</p><p>Processes running in interrupt mode in a general-purpose operating system are not preemptible. With SUSE Linux Enterprise Real Time, these interrupts have been encapsulated by kernel threads, which are interruptible, and allow the hard and soft interrupts to be preempted by user-defined higher priority processes.</p><p>In our case, if you have installed a real-time image like <code class="literal">SLE Micro RT</code>, kernel real time is already installed. From the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, you can download the real-time kernel image.</p><div id="id-1.8.7.6.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the real-time kernel, visit <a class="link" href="https://www.suse.com/products/realtime/" target="_blank">SUSE Real Time</a>.</p></div></li></ul></div></section><section class="sect1" id="kernel-args" data-id-title="Kernel arguments for low latency and high performance"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.2 </span><span class="title-name">Kernel arguments for low latency and high performance</span></span> <a title="Permalink" class="permalink" href="atip-features.html#kernel-args">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The kernel arguments are important to be configured to enable the real-time kernel to work properly giving the best performance and low latency to run telco workloads.  There are some important concepts to keep in mind when configuring the kernel arguments for this use case:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Remove <code class="literal">kthread_cpus</code> when using SUSE real-time kernel. This parameter controls on which CPUs kernel threads are created. It also controls which CPUs are allowed for PID 1 and for loading kernel modules (the kmod user-space helper). This parameter is not
recognized and does not have any effect.</p></li><li class="listitem"><p>Add <code class="literal">domain,nohz,managed_irq</code> flags to <code class="literal">isolcpus</code> kernel argument. Without any flags, <code class="literal">isolcpus</code> is equivalent to specifying only the <code class="literal">domain</code> flag. This isolates the specified CPUs from scheduling, including kernel tasks. The <code class="literal">nohz</code> flag stops the scheduler tick on the specified CPUs (if only one task is runnable on a CPU), and the <code class="literal">managed_irq</code> flag avoids routing
managed external (device) interrupts at the specified CPUs.</p></li><li class="listitem"><p>Remove <code class="literal">intel_pstate=passive</code>. This option configures <code class="literal">intel_pstate</code> to work with generic cpufreq governors, but to make this work, it disables hardware-managed P-states (<code class="literal">HWP</code>) as a side effect. To reduce the hardware latency, this option is not recommended for real-time workloads.</p></li><li class="listitem"><p>Replace <code class="literal">intel_idle.max_cstate=0 processor.max_cstate=1</code> with <code class="literal">idle=poll</code>. To avoid C-State transitions, the <code class="literal">idle=poll</code> option is used to disable the C-State transitions and keep the CPU in the highest C-State. The <code class="literal">intel_idle.max_cstate=0</code> option disables <code class="literal">intel_idle</code>, so <code class="literal">acpi_idle</code> is used, and <code class="literal">acpi_idle.max_cstate=1</code> then sets max C-state for acpi_idle.
On x86_64 architectures, the first ACPI C-State is always <code class="literal">POLL</code>, but it uses a <code class="literal">poll_idle()</code> function, which may introduce some tiny latency by reading the clock periodically, and restarting the main loop in <code class="literal">do_idle()</code> after a timeout (this also involves clearing and setting the <code class="literal">TIF_POLL</code> task flag).
In contrast, <code class="literal">idle=poll</code> runs in a tight loop, busy-waiting for a task to be rescheduled. This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</p></li><li class="listitem"><p>Disable C1E in BIOS. This option is important to disable the C1E state in the BIOS to avoid the CPU from entering the C1E state when idle. The C1E state is a low-power state that can introduce latency when the CPU is idle.</p></li><li class="listitem"><p>Add <code class="literal">nowatchdog</code> to disable the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context. When it expires (i.e. a soft lockup is detected), it will print a warning (in the hard interrupt context), running any latency targets. Even if it never expires, it goes onto the timer list, slightly increasing the overhead of every timer interrupt.
This option also disables the NMI watchdog, so NMIs cannot interfere.</p></li><li class="listitem"><p>Add <code class="literal">nmi_watchdog=0</code>. This option disables only the NMI watchdog.</p></li></ul></div><p>This is an example of the kernel argument list including the aforementioned adjustments:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</pre></div></section><section class="sect1" id="cpu-tuned-configuration" data-id-title="CPU tuned configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.3 </span><span class="title-name">CPU tuned configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cpu-tuned-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The CPU Tuned configuration allows the possibility to isolate the CPU cores to be used by the real-time kernel. It is important to prevent the OS from using the same cores as the real-time kernel, because the OS could use the cores and increase the latency in the real-time kernel.</p><p>To enable and configure this feature, the first thing is to create a profile for the CPU cores we want to isolate. In this case, we are isolating the cores <code class="literal">1-30</code> and <code class="literal">33-62</code>.</p><div class="verbatim-wrap"><pre class="screen">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-18,21-38" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</pre></div><p>Then we need to modify the GRUB option to isolate CPU cores and other important parameters for CPU usage.
The following options are important to be customized with your current hardware specifications:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>domain,nohz,managed_irq,1-18,21-38</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-18 and 21-38</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the timer tick on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-18,21-38</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-18,21-38</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,19,20,39</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the interrupts on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>idle</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>poll</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nmi_watchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option disables only the NMI watchdog.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>nowatchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "> </td><td style="text-align: left; vertical-align: top; "><p>This option disables the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context.</p></td></tr></tbody></table></div><p>With the values shown above, we are isolating 60 cores, and we are using four cores for the OS.</p><p>The following commands modify the GRUB configuration and apply the changes mentioned above to be present on the next boot:</p><p>Edit the <code class="literal">/etc/default/grub</code> file and add the parameters mentioned above:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</pre></div><p>Update the GRUB configuration:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot, the following command can be used to check the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><p>There is another script that can be used to tune the CPU configuration, which basically is doing the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Set the CPU governor to <code class="literal">performance</code>.</p></li><li class="listitem"><p>Unset the timer migration to the isolated CPUs.</p></li><li class="listitem"><p>Migrate the kdaemon threads to the housekeeping CPUs.</p></li><li class="listitem"><p>Set the isolated CPUs latency to the lowest possible value.</p></li><li class="listitem"><p>Delay the vmstat updates to 300 seconds.</p></li></ul></div><p>The script is available at <a class="link" href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.1/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh" target="_blank">SUSE ATIP Github repository - performance-settings.sh</a>.</p></section><section class="sect1" id="cni-configuration" data-id-title="CNI Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.4 </span><span class="title-name">CNI Configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cni-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-cilium" data-id-title="Cilium"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">35.4.1 </span><span class="title-name">Cilium</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-cilium">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Cilium</code> is the default CNI plug-in for ATIP.
To enable Cilium on RKE2 cluster as the default plug-in, the following configurations are required in the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- cilium</pre></div><p>This can also be specified with command-line arguments, that is, <code class="literal">--cni=cilium</code> into the server line in <code class="literal">/etc/systemd/system/rke2-server</code> file.</p><p>To use the <code class="literal">SR-IOV</code> network operator described in the next section (<a class="xref" href="atip-features.html#option2-sriov-helm">Section 35.5, “SR-IOV”</a>), use <code class="literal">Multus</code> with another CNI plug-in, like <code class="literal">Cilium</code> or <code class="literal">Calico</code>, as a secondary plug-in.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- cilium</pre></div><div id="id-1.8.7.9.2.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about CNI plug-ins, visit <a class="link" href="https://docs.rke2.io/install/network_options" target="_blank">Network Options</a>.</p></div></section></section><section class="sect1" id="sriov" data-id-title="SR-IOV"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.5 </span><span class="title-name">SR-IOV</span></span> <a title="Permalink" class="permalink" href="atip-features.html#sriov">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>SR-IOV allows a device, such as a network adapter, to separate access to its resources among various <code class="literal">PCIe</code> hardware functions.
There are different ways to deploy <code class="literal">SR-IOV</code>, and here, we show two different options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Option 1: using the <code class="literal">SR-IOV</code> CNI device plug-ins and a config map to configure it properly.</p></li><li class="listitem"><p>Option 2 (recommended): using the <code class="literal">SR-IOV</code> Helm chart from Rancher Prime to make this deployment easy.</p></li></ul></div><p id="option1-sriov-deviceplugin"><span class="strong"><strong>Option 1 - Installation of SR-IOV CNI device plug-ins and a config map to configure it properly</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the config map for the device plug-in</p></li></ul></div><p>Get the information to fill the config map from the <code class="literal">lspci</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</pre></div><p>The config map consists of a <code class="literal">JSON</code> file that describes devices using filters to discover, and creates groups for the interfaces.
The key is understanding filters and groups. The filters are used to discover the devices and the groups are used to create the interfaces.</p><p>It could be possible to set filters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>vendorID: <code class="literal">8086</code> (Intel)</p></li><li class="listitem"><p>deviceID: <code class="literal">0d5c</code> (Accelerator card)</p></li><li class="listitem"><p>driver: <code class="literal">vfio-pci</code> (driver)</p></li><li class="listitem"><p>pfNames: <code class="literal">p2p1</code> (physical interface name)</p></li></ul></div><p>It could be possible to also set filters to match more complex interface syntax, for example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>pfNames: <code class="literal">["eth1#1,2,3,4,5,6"]</code> or <code class="literal">[eth1#1-6]</code> (physical interface name)</p></li></ul></div><p>Related to the groups, we could create a group for the <code class="literal">FEC</code> card and another group for the <code class="literal">Intel</code> card, even creating a prefix depending on our use case:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>resourceName: <code class="literal">pci_sriov_net_bh_dpdk</code></p></li><li class="listitem"><p>resourcePrefix: <code class="literal">Rancher.io</code></p></li></ul></div><p>There are a lot of combinations to discover and create the resource group to allocate some <code class="literal">VFs</code> to the pods.</p><div id="id-1.8.7.10.16" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the filters and groups, visit <a class="link" href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank">sr-iov network device plug-in</a>.</p></div><p>After setting the filters and groups to match the interfaces depending on the hardware and the use case, the following config map shows an example to be used:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the <code class="literal">daemonset</code> file to deploy the device plug-in.</p></li></ul></div><p>The device plug-in supports several architectures (<code class="literal">arm</code>, <code class="literal">amd</code>, <code class="literal">ppc64le</code>), so the same file can be used for different architectures deploying several <code class="literal">daemonset</code> for each architecture.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>After applying the config map and the <code class="literal">daemonset</code>, the device plug-in will be deployed and the interfaces will be discovered and available for the pods.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</pre></div></li><li class="listitem"><p>Check the interfaces discovered and available in the nodes to be used by the pods:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</pre></div></li><li class="listitem"><p>The <code class="literal">FEC</code> is <code class="literal">intel.com/intel_fec_5g</code> and the value is 1.</p></li><li class="listitem"><p>The <code class="literal">VF</code> is <code class="literal">intel.com/intel_sriov_odu</code> or <code class="literal">intel.com/intel_sriov_oru</code> if you deploy it with a device plug-in and the config map without Helm charts.</p></li></ul></div><div id="id-1.8.7.10.23" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>If there are no interfaces here, it makes little sense to continue because the interface will not be available for pods. Review the config map and filters to solve the issue first.</p></div><p id="option2-sriov-helm"><span class="strong"><strong>Option 2 (recommended) - Installation using Rancher using Helm chart for SR-IOV CNI and device plug-ins</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Get Helm if not present:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install SR-IOV.</p></li></ul></div><p>This part could be done in two ways, using the <code class="literal">CLI</code> or using the <code class="literal">Rancher UI</code>.</p><div class="variablelist"><dl class="variablelist"><dt id="id-1.8.7.10.29.1"><span class="term">Install Operator from CLI</span></dt><dd><div class="verbatim-wrap"><pre class="screen">helm install sriov-crd oci://registry.suse.com/edge/3.1/sriov-crd-chart -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/3.1/sriov-network-operator-chart -n sriov-network-operator</pre></div></dd><dt id="id-1.8.7.10.29.2"><span class="term">Install Operator from Rancher UI</span></dt><dd><p>Once your cluster is installed, and you have access to the <code class="literal">Rancher UI</code>, you can install the <code class="literal">SR-IOV Operator</code> from the <code class="literal">Rancher UI</code> from the apps tab:</p></dd></dl></div><div id="id-1.8.7.10.30" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Make sure you select the right namespace to install the operator, for example, <code class="literal">sriov-network-operator</code>.</p></div><p>+
image::features_sriov.png[sriov.png]</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the  deployed resources crd and pods:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the label in the nodes.</p></li></ul></div><p>With all resources running, the label appears automatically in your node:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the <code class="literal">daemonset</code> to see the new <code class="literal">sriov-network-config-daemon</code> and <code class="literal">sriov-rancher-nfd-worker</code> as active and ready:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</pre></div><p>In a few minutes (can take up to 10 min to be updated), the nodes are detected and configured with the <code class="literal">SR-IOV</code> capabilities:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the interfaces detected.</p></li></ul></div><p>The interfaces discovered should be the PCI address of the network device. Check this information with the <code class="literal">lspci</code> command in the host.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</pre></div><div id="id-1.8.7.10.44" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>If your interface is not detected here, ensure that it is present in the next config map:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</pre></div><p>If your device is not there, edit the config map, adding the right values to be discovered (should be necessary to restart the <code class="literal">sriov-network-config-daemon</code> daemonset).</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">NetworkNode Policy</code> to configure the <code class="literal">VFs</code>.</p></li></ul></div><p>Some <code class="literal">VFs</code> (<code class="literal">numVfs</code>) from the device (<code class="literal">rootDevices</code>) will be created, and it will be configured with the driver <code class="literal">deviceType</code> and the <code class="literal">MTU</code>:</p><div id="id-1.8.7.10.47" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">resourceName</code> field must not contain any special characters and must be unique across the cluster.
The example uses the <code class="literal">deviceType: vfio-pci</code> because <code class="literal">dpdk</code> will be used in combination with <code class="literal">sr-iov</code>. If you don’t use <code class="literal">dpdk</code>, the deviceType should be <code class="literal">deviceType: netdevice</code> (default value).</p></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Validate configurations:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the sr-iov network (optional, just in case a different network is needed):</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the network created:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</pre></div></section><section class="sect1" id="dpdk" data-id-title="DPDK"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.6 </span><span class="title-name">DPDK</span></span> <a title="Permalink" class="permalink" href="atip-features.html#dpdk">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">DPDK</code> (Data Plane Development Kit) is a set of libraries and drivers for fast packet processing. It is used to accelerate packet processing workloads running on a wide variety of CPU architectures.
The DPDK includes data plane libraries and optimized network interface controller (<code class="literal">NIC</code>) drivers for the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A queue manager implements lockless queues.</p></li><li class="listitem"><p>A buffer manager pre-allocates fixed size buffers.</p></li><li class="listitem"><p>A memory manager allocates pools of objects in memory and uses a ring to store free objects; ensures that objects are spread equally on all <code class="literal">DRAM</code> channels.</p></li><li class="listitem"><p>Poll mode drivers (<code class="literal">PMD</code>) are designed to work without asynchronous notifications, reducing overhead.</p></li><li class="listitem"><p>A packet framework as a set of libraries that are helpers to develop packet processing.</p></li></ol></div><p>The following steps will show how to enable <code class="literal">DPDK</code> and how to create <code class="literal">VFs</code> from the <code class="literal">NICs</code> to be used by the <code class="literal">DPDK</code> interfaces:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install the <code class="literal">DPDK</code> package:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters:</p></li></ul></div><p>To use DPDK, employ some drivers to enable certain parameters in the kernel:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option enables the use  of the <code class="literal">vfio</code> driver for the DPDK interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables the use of <code class="literal">vfio</code> for <code class="literal">VFs</code>.</p></td></tr></tbody></table></div><p>To enable the parameters, add them to the <code class="literal">/etc/default/grub</code> file:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load <code class="literal">vfio-pci</code> kernel module and enable <code class="literal">SR-IOV</code> on the <code class="literal">NICs</code>:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create some virtual functions (<code class="literal">VFs</code>) from the <code class="literal">NICs</code>.</p></li></ul></div><p>To create for <code class="literal">VFs</code>, for example, for two different <code class="literal">NICs</code>, the following commands are required:</p><div class="verbatim-wrap"><pre class="screen">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the new VFs with the <code class="literal">vfio-pci</code> driver:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the configuration is correctly applied:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</pre></div></section><section class="sect1" id="acceleration" data-id-title="vRAN acceleration (Intel ACC100/ACC200)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.7 </span><span class="title-name">vRAN acceleration (<code class="literal">Intel ACC100/ACC200</code>)</span></span> <a title="Permalink" class="permalink" href="atip-features.html#acceleration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>As communications service providers move from 4 G to 5 G networks, many are adopting virtualized radio access network (<code class="literal">vRAN</code>) architectures for higher channel capacity and easier deployment of edge-based services and applications. vRAN solutions are ideally located to deliver low-latency services with the flexibility to increase or decrease capacity based on the volume of real-time traffic and demand on the network.</p><p>One of the most compute-intensive 4 G and 5 G workloads is RAN layer 1 (<code class="literal">L1</code>) <code class="literal">FEC</code>, which resolves data transmission errors over unreliable or noisy communication channels. <code class="literal">FEC</code> technology detects and corrects a limited number of errors in 4 G or 5 G data, eliminating the need for retransmission. Since the <code class="literal">FEC</code> acceleration transaction does not contain cell state information, it can be easily virtualized, enabling pooling benefits and easy cell migration.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To enable the <code class="literal">vRAN</code> acceleration, we need to enable the following kernel parameters (if not present yet):</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option enables the use of vfio for the DPDK interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables the use of vfio for VFs.</p></td></tr></tbody></table></div><p>Modify the GRUB file <code class="literal">/etc/default/grub</code> to add them to the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To verify that the parameters are applied after the reboot, check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load vfio-pci kernel modules to enable the <code class="literal">vRAN</code> acceleration:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Get interface information Acc100:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the physical interface (<code class="literal">PF</code>) with <code class="literal">vfio-pci</code> driver:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the virtual functions (<code class="literal">VFs</code>) from the physical interface (<code class="literal">PF</code>).</p></li></ul></div><p>Create 2 <code class="literal">VFs</code> from the <code class="literal">PF</code> and bind with <code class="literal">vfio-pci</code> following the next steps:</p><div class="verbatim-wrap"><pre class="screen">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Configure acc100 with the proposed configuration file:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the new VFs created from the FEC PF:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</pre></div></section><section class="sect1" id="huge-pages" data-id-title="Huge pages"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.8 </span><span class="title-name">Huge pages</span></span> <a title="Permalink" class="permalink" href="atip-features.html#huge-pages">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When a process uses <code class="literal">RAM</code>, the <code class="literal">CPU</code> marks it as used by that process. For efficiency, the <code class="literal">CPU</code> allocates <code class="literal">RAM</code> in chunks <code class="literal">4K</code> bytes is the default value on many platforms. Those chunks are named pages. Pages can be swapped to disk, etc.</p><p>Since the process address space is virtual, the <code class="literal">CPU</code> and the operating system need to remember which pages belong to which process, and where each page is stored. The greater the number of pages, the longer the search for memory mapping. When a process uses <code class="literal">1 GB</code> of memory, that is 262144 entries to look up (<code class="literal">1 GB</code> / <code class="literal">4 K</code>). If a page table entry consumes 8 bytes, that is <code class="literal">2 MB</code> (262144 * 8) to look up.</p><p>Most current <code class="literal">CPU</code> architectures support larger-than-default pages, which give the <code class="literal">CPU/OS</code> fewer entries to look up.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To enable the huge pages, we should add the next kernel parameters:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows to set the size of huge pages to 1 G</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This is the number of huge pages defined before</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; "><p>This is the default value to get the huge pages</p></td></tr></tbody></table></div><p>Modify the GRUB file <code class="literal">/etc/default/grub</code> to add them to the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot, you can check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Using huge pages</p></li></ul></div><p>To use the huge pages, we need to mount them:</p><div class="verbatim-wrap"><pre class="screen">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</pre></div><p>Deploy a Kubernetes workload, creating the resources and the volumes:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</pre></div><div class="verbatim-wrap highlight yaml"><pre class="screen">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</pre></div></section><section class="sect1" id="cpu-pinning-configuration" data-id-title="CPU pinning configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.9 </span><span class="title-name">CPU pinning configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cpu-pinning-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Requirements</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Must have the <code class="literal">CPU</code> tuned to the performance profile covered in this section (<a class="xref" href="atip-features.html#cpu-tuned-configuration" title="35.3. CPU tuned configuration">Section 35.3, “CPU tuned configuration”</a>).</p></li><li class="listitem"><p>Must have the <code class="literal">RKE2</code> cluster kubelet configured with the CPU management arguments adding the following block (as an example) to the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p></li></ol></div></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">kubelet-arg:
- "cpu-manager=true"
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=1"
- "system-reserved=cpu=1"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Using CPU pinning on Kubernetes</p></li></ul></div><p>There are three ways to use that feature using the <code class="literal">Static Policy</code> defined in kubelet depending on the requests and limits you define on your workload:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">BestEffort</code> QoS Class: If you do not define any request or limit for <code class="literal">CPU</code>, the pod is scheduled on the first <code class="literal">CPU</code> available on the system.</p><p>An example of using the <code class="literal">BestEffort</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx</pre></div></li><li class="listitem"><p><code class="literal">Burstable</code> QoS Class: If you define a request for CPU, which is not equal to the limits, or there is no CPU request.</p><p>Examples of using the <code class="literal">Burstable</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</pre></div><p>or</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</pre></div></li><li class="listitem"><p><code class="literal">Guaranteed</code> QoS Class: If you define a request for CPU, which is equal to the limits.</p><p>An example of using the <code class="literal">Guaranteed</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</pre></div></li></ol></div></section><section class="sect1" id="numa-aware-scheduling" data-id-title="NUMA-aware scheduling"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.10 </span><span class="title-name">NUMA-aware scheduling</span></span> <a title="Permalink" class="permalink" href="atip-features.html#numa-aware-scheduling">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Non-Uniform Memory Access or Non-Uniform Memory Architecture (<code class="literal">NUMA</code>) is a physical memory design used in <code class="literal">SMP</code> (multiprocessors) architecture, where the memory access time depends on the memory location relative to a processor. Under <code class="literal">NUMA</code>, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors.</p><section class="sect2" id="id-identifying-numa-nodes" data-id-title="Identifying NUMA nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">35.10.1 </span><span class="title-name">Identifying NUMA nodes</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-identifying-numa-nodes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To identify the <code class="literal">NUMA</code> nodes, on your system use the following command:</p><div class="verbatim-wrap"><pre class="screen">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</pre></div><div id="id-1.8.7.15.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For this example, we have only one <code class="literal">NUMA</code> node showing 64 <code class="literal">CPUs</code>.</p><p><code class="literal">NUMA</code> needs to be enabled in the <code class="literal">BIOS</code>. If <code class="literal">dmesg</code> does not have records of NUMA initialization during the bootup, then <code class="literal">NUMA</code>-related messages in the kernel ring buffer might have been overwritten.</p></div></section></section><section class="sect1" id="metal-lb-configuration" data-id-title="Metal LB"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.11 </span><span class="title-name">Metal LB</span></span> <a title="Permalink" class="permalink" href="atip-features.html#metal-lb-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">MetalLB</code> is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols like <code class="literal">L2</code> and <code class="literal">BGP</code> as advertisement protocols. It is a network load balancer that can be used to expose services in a Kubernetes cluster to the outside world due to the need to use Kubernetes Services type <code class="literal">LoadBalancer</code> with bare-metal.</p><p>To enable <code class="literal">MetalLB</code> in the <code class="literal">RKE2</code> cluster, the following steps are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install <code class="literal">MetalLB</code> using the following command:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/3.2/metallb-chart
  targetNamespace: metallb-system
  version: 0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/3.2/endpoint-copier-operator-chart
  targetNamespace: endpoint-copier-operator
  version: 0.2.1
  createNamespace: true
EOF</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">IpAddressPool</code> and the <code class="literal">L2advertisement</code> configuration:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the endpoint service to expose the <code class="literal">VIP</code>:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the <code class="literal">VIP</code> is created and the <code class="literal">MetalLB</code> pods are running:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get svc -n default
$ kubectl get pods -n default</pre></div></section><section class="sect1" id="private-registry" data-id-title="Private registry configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">35.12 </span><span class="title-name">Private registry configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#private-registry">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Containerd</code> can be configured to connect to private registries and use them to pull private images on each node.</p><p>Upon startup, <code class="literal">RKE2</code> checks if a <code class="literal">registries.yaml</code> file exists at <code class="literal">/etc/rancher/rke2/</code> and instructs <code class="literal">containerd</code> to use any registries defined in the file. If you wish to use a private registry, create this file as root on each node that will use the registry.</p><p>To add the private registry, create the file <code class="literal">/etc/rancher/rke2/registries.yaml</code> with the following content:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>or without authentication:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>For the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.</p><div id="id-1.8.7.17.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about this, please check <a class="link" href="https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file" target="_blank">containerd registry configuration rke2</a>.</p></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="atip-management-cluster.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 34 </span>Setting up the management cluster</span></a> </div><div><a class="pagination-link next" href="atip-automated-provisioning.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 36 </span>Fully automated directed network provisioning</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip-features.html#kernel-image-for-real-time"><span class="title-number">35.1 </span><span class="title-name">Kernel image for real time</span></a></span></li><li><span class="section"><a href="atip-features.html#kernel-args"><span class="title-number">35.2 </span><span class="title-name">Kernel arguments for low latency and high performance</span></a></span></li><li><span class="section"><a href="atip-features.html#cpu-tuned-configuration"><span class="title-number">35.3 </span><span class="title-name">CPU tuned configuration</span></a></span></li><li><span class="section"><a href="atip-features.html#cni-configuration"><span class="title-number">35.4 </span><span class="title-name">CNI Configuration</span></a></span></li><li><span class="section"><a href="atip-features.html#sriov"><span class="title-number">35.5 </span><span class="title-name">SR-IOV</span></a></span></li><li><span class="section"><a href="atip-features.html#dpdk"><span class="title-number">35.6 </span><span class="title-name">DPDK</span></a></span></li><li><span class="section"><a href="atip-features.html#acceleration"><span class="title-number">35.7 </span><span class="title-name">vRAN acceleration (<code class="literal">Intel ACC100/ACC200</code>)</span></a></span></li><li><span class="section"><a href="atip-features.html#huge-pages"><span class="title-number">35.8 </span><span class="title-name">Huge pages</span></a></span></li><li><span class="section"><a href="atip-features.html#cpu-pinning-configuration"><span class="title-number">35.9 </span><span class="title-name">CPU pinning configuration</span></a></span></li><li><span class="section"><a href="atip-features.html#numa-aware-scheduling"><span class="title-number">35.10 </span><span class="title-name">NUMA-aware scheduling</span></a></span></li><li><span class="section"><a href="atip-features.html#metal-lb-configuration"><span class="title-number">35.11 </span><span class="title-name">Metal LB</span></a></span></li><li><span class="section"><a href="atip-features.html#private-registry"><span class="title-number">35.12 </span><span class="title-name">Private registry configuration</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2025</span></div></div></footer></body></html>