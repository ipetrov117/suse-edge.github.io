<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | SUSE Adaptive Telco Infrastructure Platform (ATIP)</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="SUSE Adaptive Telco Infrastructure Platform (ATIP)"/>
<meta name="description" content="SUSE Adaptive Telco Infrastructure Platform (ATIP) is a Telco-optimized edge computing platform that enables telecom companies to innovate and accele…"/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 21. SUSE Adaptive Telco Infrastructure Platform (ATIP)"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="SUSE Adaptive Telco Infrastructure Platform (ATIP)"/>
<meta property="og:description" content="SUSE Adaptive Telco Infrastructure Platform (ATIP) is a Tel…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="SUSE Adaptive Telco Infrastructure Platform (ATIP)"/>
<meta name="twitter:description" content="SUSE Adaptive Telco Infrastructure Platform (ATIP) is a Tel…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
    "inLanguage": "en",
    

    "headline": "SUSE Adaptive Telco Infrastructure Platform (ATIP)",
  
    "description": "SUSE Adaptive Telco Infrastructure Platform (ATIP)",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-04-02T00:00+02:00",
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="id-product-documentation.html" title="Part V. Product Documentation"/><link rel="next" href="id-appendix.html" title="Appendix A. Appendix"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-product-documentation.html">Product Documentation</a><span> / </span><a class="crumb" href="atip.html">SUSE Adaptive Telco Infrastructure Platform (ATIP)</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="id-suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge Documentation</span></a></li><li><a href="id-quickstarts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quickstarts</span></a><ol><li><a href="quickstart-eib.html" class=" "><span class="title-number">1 </span><span class="title-name">Standalone Clusters with Edge Image Builder</span></a></li><li><a href="id-bmc-automated-deployments-with-metal3.html" class=" "><span class="title-number">2 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">3 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li></ol></li><li><a href="id-components-used-2.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components Used</span></a><ol><li><a href="components-elemental.html" class=" "><span class="title-number">4 </span><span class="title-name">Elemental</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">5 </span><span class="title-name">Fleet</span></a></li><li><a href="components-longhorn.html" class=" "><span class="title-number">6 </span><span class="title-name">Longhorn</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">7 </span><span class="title-name">Metal³</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">8 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-neuvector.html" class=" "><span class="title-number">9 </span><span class="title-name">NeuVector</span></a></li><li><a href="components-rancher.html" class=" "><span class="title-number">10 </span><span class="title-name">Rancher</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">11 </span><span class="title-name">SLE Micro</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">12 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">13 </span><span class="title-name">RKE2</span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">14 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="id-edge-virtualization.html" class=" "><span class="title-number">15 </span><span class="title-name">Edge Virtualization</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">16 </span><span class="title-name">MetalLB on K3s (using L2)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">17 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Third Party Integration</span></a><ol><li><a href="integrations-linkert.html" class=" "><span class="title-number">18 </span><span class="title-name">Using the Linkerd Service Mesh</span></a></li><li><a href="integrations-nats.html" class=" "><span class="title-number">19 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-sle-micro.html" class=" "><span class="title-number">20 </span><span class="title-name">NVIDIA GPU’s on SLE Micro</span></a></li></ol></li><li class="active"><a href="id-product-documentation.html" class="has-children you-are-here"><span class="title-number">V </span><span class="title-name">Product Documentation</span></a><ol><li><a href="atip.html" class=" you-are-here"><span class="title-number">21 </span><span class="title-name">SUSE Adaptive Telco Infrastructure Platform (ATIP)</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">A </span><span class="title-name">Appendix</span></a><ol><li><a href="id-appendix.html#id-terminology" class=" "><span class="title-number"> </span><span class="title-name">Terminology</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip" data-id-title="SUSE Adaptive Telco Infrastructure Platform (ATIP)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">21 </span><span class="title-name">SUSE Adaptive Telco Infrastructure Platform (ATIP)</span></span> <a title="Permalink" class="permalink" href="atip.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>SUSE Adaptive Telco Infrastructure Platform (<code class="literal">ATIP</code>) is a Telco-optimized edge computing platform that enables telecom companies to innovate and accelerate modernization of their networks.</p><p>ATIP is a complete Telco cloud stack for hosting CNFs such as 5G Packet Core, Cloud RAN.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Automates zero-touch rollout and lifecycle management of complex edge stack configurations at Telco scale.</p></li><li class="listitem"><p>Continuously assureds quality on Telco-grade hardware, using Telco-specific configurations and workloads.</p></li><li class="listitem"><p>Consists of components that are purpose built for the edge and hence have smaller footprint and higher performance per Watt.</p></li><li class="listitem"><p>Maintains a flexible platform strategy with vendor-neutral APIs and 100% open source.</p></li></ul></div><section class="sect1" id="id-concept-architecture" data-id-title="Concept Architecture"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.1 </span><span class="title-name">Concept &amp; Architecture</span></span> <a title="Permalink" class="permalink" href="atip.html#id-concept-architecture">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>SUSE ATIP is a platform designed for hosting modern, cloud native, Telco applications at scale from core to edge.</p><p>This page explains the architecture and components used in ATIP. Knowledge of this will assist in deploying and using ATIP.</p><section class="sect2" id="id-atip-architecture" data-id-title="ATIP Architecture"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.1.1 </span><span class="title-name">ATIP Architecture</span></span> <a title="Permalink" class="permalink" href="atip.html#id-atip-architecture">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following diagram shows the high level architecture of ATIP:</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-architecture1.png"><img src="images/product-atip-architecture1.png" width="NaN" alt="product atip architecture1" title="product atip architecture1"/></a></div></div></section><section class="sect2" id="id-components" data-id-title="Components"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.1.2 </span><span class="title-name">Components</span></span> <a title="Permalink" class="permalink" href="atip.html#id-components">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>There are two different blocks, the management stack and the runtime stack:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Management stack</strong></span>: This is the part of ATIP that is used to manage the provision and lifecycle of the runtime stacks. It includes the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Multi-cluster management in public and private cloud environments with Rancher Prime (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p></li><li class="listitem"><p>Bare metal support with Metal3 (<span class="intraxref">Book “SUSE Edge Documentation”</span>), MetalLB (<span class="intraxref">Book “SUSE Edge Documentation”</span>) and <code class="literal">CAPI</code> (Cluster API) infrastructure providers</p></li><li class="listitem"><p>Comprehensive tenant isolation and <code class="literal">IDP</code> (Identity Provider) integrations.</p></li><li class="listitem"><p>Large marketplace of 3rd party integrations and extensions</p></li><li class="listitem"><p>Vendor-neutral API and rich ecosystem of providers</p></li><li class="listitem"><p>Control the SLE Micro transactional updates</p></li><li class="listitem"><p>GitOps Engine for managing the lifecycle of the clusters using Git repositories with Fleet (<span class="intraxref">Book “SUSE Edge Documentation”</span>).</p></li></ul></div></li><li class="listitem"><p><span class="strong"><strong>Runtime stack</strong></span>: This is the part of ATIP that is used to run the workloads.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kubernetes with secure and lightweight distributions like K3S (<span class="intraxref">Book “SUSE Edge Documentation”</span>) and RKE2 (<span class="intraxref">Book “SUSE Edge Documentation”</span>) (<code class="literal">RKE2</code> is secure hardened, certified, and optimized for government use and regulated industries)</p></li><li class="listitem"><p>Container Security with NeuVector (<span class="intraxref">Book “SUSE Edge Documentation”</span>) to enable security features like image vulnerability scanning, deep packet inspection and automatic intra-cluster traffic control.</p></li><li class="listitem"><p>Block Storage with Longhorn (<span class="intraxref">Book “SUSE Edge Documentation”</span>) to enable a simple and easy way to use a cloud native storage solution.</p></li><li class="listitem"><p>Optimized Operating System with SLE Micro (<span class="intraxref">Book “SUSE Edge Documentation”</span>) to enable a secure, lightweight, and immutable (transactional filesystem) OS for running containers. SLE Micro is available on <code class="literal">aarch64</code> and <code class="literal">x86_64</code> architectures, and it also supports <code class="literal">Real-Time Kernel</code> for Telco and edge use cases.</p></li></ul></div></li></ul></div></section><section class="sect2" id="id-example-deployment-flows" data-id-title="Example deployment flows"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.1.3 </span><span class="title-name">Example deployment flows</span></span> <a title="Permalink" class="permalink" href="atip.html#id-example-deployment-flows">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following are some high level examples of workflows to understand the relationship between the management and the runtime components.
<code class="literal">ZTP</code> (Zero Touch Provisioning) is the workflow that enables the deployment of a new downstream cluster with all the components pre-configured and ready to run workloads without any manual intervention.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Example 1: Deploying a new management cluster with all components installed</p></li></ul></div><p>Using the <a class="link" href="../components/eib.xml" target="_blank">Edge Image Builder</a> to create a new <code class="literal">ISO</code> image with the management stack included. You can then use this `ISO to install a new management cluster on VMs or Bare metal.</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-architecture2.png"><img src="images/product-atip-architecture2.png" width="NaN" alt="product atip architecture2" title="product atip architecture2"/></a></div></div><div id="id-1.7.3.5.6.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about how to deploy a new management cluster, see the <a class="link" href="atip-management-cluster.xml" target="_blank">ATIP Management Cluster</a> guide.</p></div><div id="id-1.7.3.5.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about how to use the Edge Image Builder, see the Edge Image Builder (<span class="intraxref">Book “SUSE Edge Documentation”</span>) guide.</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Example 2: Deploying a single-node downstream cluster with Telco profiles to enable it to run Telco workloads</p></li></ul></div><p>Once we have the management cluster up and running, we can use it to deploy a single-node downstream cluster with all Telco capabilities enabled and configured using the <code class="literal">ZTP</code> workflow.</p><p>The following diagram shows the high level workflow to deploy it:</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-architecture3.png"><img src="images/product-atip-architecture3.png" width="NaN" alt="product atip architecture3" title="product atip architecture3"/></a></div></div><div id="id-1.7.3.5.6.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about how to deploy a downstream cluster, see the <a class="link" href="atip-automated-provision.xml" target="_blank">ATIP Automated Provision</a> guide.</p></div><div id="id-1.7.3.5.6.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about Telco features, see the <a class="link" href="atip-features.xml" target="_blank">ATIP Telco Features</a> guide.</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Example 3: Deploying a high availability downstream cluster using MetalLB as a Load Balancer</p></li></ul></div><p>Once we have the management cluster up and running, we can use it to deploy a high availability downstream cluster with <code class="literal">MetalLB</code> as a load balancer using the <code class="literal">ZTP</code> workflow.</p><p>The following diagram shows the high level workflow to deploy it:</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-architecture4.png"><img src="images/product-atip-architecture4.png" width="NaN" alt="product atip architecture4" title="product atip architecture4"/></a></div></div><div id="id-1.7.3.5.6.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about how to deploy a downstream cluster, see the <a class="link" href="atip-automated-provision.xml" target="_blank">ATIP Automated Provision</a> guide.</p></div><div id="id-1.7.3.5.6.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">MetalLB</code>, see the MetalLB (<span class="intraxref">Book “SUSE Edge Documentation”</span>) guide.</p></div></section></section><section class="sect1" id="id-requirements-assumptions" data-id-title="Requirements Assumptions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.2 </span><span class="title-name">Requirements &amp; Assumptions</span></span> <a title="Permalink" class="permalink" href="atip.html#id-requirements-assumptions">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-hardware" data-id-title="Hardware"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.1 </span><span class="title-name">Hardware</span></span> <a title="Permalink" class="permalink" href="atip.html#id-hardware">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The hardware requirements for the ATIP nodes are based on the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Managment Cluster</strong></span>: The management cluster contains components like <code class="literal">SLE Micro</code>, <code class="literal">RKE2</code>, <code class="literal">Rancher Prime</code>, <code class="literal">Metal<sup>3</sup></code>, and it is used to manage a number of downstream clusters. Depending on the number of downstream clusters to be managed, the hardware requirements for the server could vary.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Minimum requirements for the server (<code class="literal">VM</code> or <code class="literal">Bare Metal</code>) are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>RAM: 8GB Minimum (we recommend at least 16GB)</p></li><li class="listitem"><p>CPU: 2 Minimum (we recommend at least 4CPU)</p></li></ul></div></li></ul></div></li><li class="listitem"><p><span class="strong"><strong>Downstream Clusters</strong></span>: The downstream clusters are the clusters deployed on the ATIP nodes to run Telco workloads. Some specific requirements are needed to enable some Telco capabilities like <code class="literal">SR-IOV</code>, <code class="literal">CPU Performance Optimization</code>, etc.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>SR-IOV: to be able to attach VFs (Virtual Functions) in pass-through mode to CNFs/VNFs, the NIC must support SR-IOV and VT-d/AMD-Vi be enabled in the BIOS.</p></li><li class="listitem"><p>CPU Processors: To run some specific Telco workloads the CPU Processor model should be adapted to enable most of the features available on this reference <a class="link" href="atip-features.xml" target="_blank">table</a>.</p></li><li class="listitem"><p>Firmware requirements for installing with virtual media:</p></li></ul></div></li></ul></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Server Hardware</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>BMC Model</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Management</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Dell hardware</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>15th Generation</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>iDRAC9</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Supermicro hardware</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>01.00.25</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Supermicro SMC - redfish</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>HPE hardware</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1.50</p></td><td style="text-align: left; vertical-align: top; "><p>iLO6</p></td></tr></tbody></table></div></section><section class="sect2" id="id-network" data-id-title="Network"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.2 </span><span class="title-name">Network</span></span> <a title="Permalink" class="permalink" href="atip.html#id-network">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>As a reference for the network architecture, the following diagram shows a typical network architecture for a Telco environment:</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-requirement1.png"><img src="images/product-atip-requirement1.png" width="NaN" alt="product atip requirement1" title="product atip requirement1"/></a></div></div><p>The network architecture is based on the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Management Network</strong></span>: This network is used for the management of the ATIP nodes. It is used for the out-of-band management. Usually this network is also connected to a separate management switch, but it can be connected to the same service switch using VLANs to isolate the traffic.</p></li><li class="listitem"><p><span class="strong"><strong>Control Plane Network</strong></span>: This network is used for the communication between the ATIP nodes and the services that are running on them. This network is also used for the communication between the ATIP nodes and the external services, like the <code class="literal">DHCP</code> or <code class="literal">DNS</code> servers. In some cases, for connected environments, the switch/router should be able to handle traffic through Internet.</p></li><li class="listitem"><p><span class="strong"><strong>Other Networks</strong></span>: In some cases, the ATIP nodes could be connected to other networks for specific customer purposes.</p></li></ul></div><div id="id-1.7.3.6.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>To use the <code class="literal">ATIP ZTP workflow</code>, the <code class="literal">controlplane network</code> and the <code class="literal">management network</code> should be connected in order to provision the ATIP nodes using <code class="literal">metal<sup>3</sup></code>. The management cluster will provide the image to be provisioned through the <code class="literal">BMC</code> so the communication between the management cluster and the <code class="literal">BMC</code> is required.</p></div></section><section class="sect2" id="id-services-dhcp-dns-etc" data-id-title="Services (DHCP, DNS, etc)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.3 </span><span class="title-name">Services (DHCP, DNS, etc)</span></span> <a title="Permalink" class="permalink" href="atip.html#id-services-dhcp-dns-etc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Some external services like <code class="literal">DHCP</code>, <code class="literal">DNS</code>, etc…​ could be required depending on the kind of environment will be deployed:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Connected Environment</strong></span>: In this case, the ATIP nodes will be connected to the Internet (via routing L3 protocols) and the external services will be provided by the customer.</p></li><li class="listitem"><p><span class="strong"><strong>Disconnected / Air-gap Environment</strong></span>: In this case, the ATIP nodes will not have Internet IP connectivity (via routing L3 protocols) and the configuration for static IPs will be configured during the bootstrap process on the ATIP ZTP workflow.</p></li><li class="listitem"><p><span class="strong"><strong>File Server</strong></span>: A file server is used to store the ISO images to be provisioned on the ATIP nodes during the ZTP workflow. <code class="literal">metal<sup>3</sup></code> helm chart can deploy a media server to store the ISO images (check the following <a class="link" href="atip-management-cluster.xml#metal3-media-server" target="_blank">section</a>), but it is also possible to use an existing customer file server.</p></li></ul></div></section><section class="sect2" id="id-disable-rebootmgr" data-id-title="Disable rebootmgr"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.4 </span><span class="title-name">Disable rebootmgr</span></span> <a title="Permalink" class="permalink" href="atip.html#id-disable-rebootmgr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">rebootmgr</code> is a service which allow to configure a strategy for reboot in case system have some updates available pending.
For Telco workloads is really important to disable or configure properly the <code class="literal">rebootmgr</code> service in order to avoid the reboot of the nodes in case of updates scheduled by the system, to avoid any impact on the services running on the nodes.</p><div id="id-1.7.3.6.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">rebootmgr</code>, please check <a class="link" href="https://github.com/SUSE/rebootmgr" target="_blank">rebootmgr GitHub repository</a>.</p></div><p>You could verify the strategic being used as:</p><div class="verbatim-wrap"><pre class="screen">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</pre></div><p>and you could disable it as:</p><div class="verbatim-wrap"><pre class="screen">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</pre></div><p>or using the <code class="literal">rebootmgrctl</code> command:</p><div class="verbatim-wrap"><pre class="screen">rebootmgrctl strategy off</pre></div><div id="id-1.7.3.6.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>This configuration to set the <code class="literal">rebootmgr</code> strategy, can be automated using the <code class="literal">ATIP ZTP workflow</code>. For more information, please check the <a class="link" href="atip-automated-provision.xml" target="_blank">ATIP Automated Provision</a> document.</p></div></section></section><section class="sect1" id="id-setting-up-the-management-cluster" data-id-title="Setting up the Management Cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.3 </span><span class="title-name">Setting up the Management Cluster</span></span> <a title="Permalink" class="permalink" href="atip.html#id-setting-up-the-management-cluster">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-introduction" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="atip.html#id-introduction">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The Management Cluster is the part of ATIP that is used to manage the provision and lifecycle of the runtime stacks.
From a technical point of view, the management cluster contains the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">SUSE Linux Enterprise Micro</code> as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.</p></li><li class="listitem"><p><code class="literal">RKE2</code> as the Kubernetes cluster. Depending on the use case, it can be configured to use specific CNI plugins, such as <code class="literal">Multus</code>, <code class="literal">Cilium</code> etc.</p></li><li class="listitem"><p><code class="literal">Rancher</code> as the management platform to manage the lifecycle of the clusters.</p></li><li class="listitem"><p><code class="literal">Metal<sup>3</sup></code> as the component to manage the lifecycle of the bare metal nodes.</p></li><li class="listitem"><p><code class="literal">CAPI</code> as the component to manage the lifecycle of the Kubernetes clusters (downstream clusters). In the case of ATIP, also the <code class="literal">RKE2 CAPI Provider</code> is used to manage the lifecycle of the RKE2 clusters (downstream clusters).</p></li></ul></div><p>With all components mentioned above, the management cluster is able to manage the lifecycle of the runtime stacks, using a declarative approach and the <code class="literal">Zero Touch Provisioning</code> to manage the infrastructure and the applications.</p><div id="id-1.7.3.7.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">SUSE Linux Enterprise Micro</code>, please check: Components - SUSE Linux Enterprise Micro (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">RKE2</code>, please check: Components - RKE2 (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">Rancher</code>, please check: Components - Rancher (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">Metal3</code>, please check: Components - Metal3 (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p></div></section><section class="sect2" id="id-steps-to-setup-the-management-cluster" data-id-title="Steps to setup the Management Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.2 </span><span class="title-name">Steps to setup the Management Cluster</span></span> <a title="Permalink" class="permalink" href="atip.html#id-steps-to-setup-the-management-cluster">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following steps are necessary to set up the management cluster (using a single-node):</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-mgmtcluster1.png"><img src="images/product-atip-mgmtcluster1.png" width="NaN" alt="product atip mgmtcluster1" title="product atip mgmtcluster1"/></a></div></div><p>There are two main steps to set up the management cluster using a declarative approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Image Creation</strong></span>: The first step is to create the image to be used for the management cluster with all the necessary configurations. This image will be generated using <code class="literal">Edge Image Builder</code> and this step can be done using a laptop, server, VM or any other x86_64 system with a container runtime installed.</p></li><li class="listitem"><p><span class="strong"><strong>Management Cluster Provision</strong></span>: This step covers the installation of the management cluster using the image created in the previous step, and it will install all components mentioned in the introduction.</p></li></ul></div><div id="id-1.7.3.7.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">Edge Image Builder</code>, please check: <a class="link" href="../components/eib.xml" target="_blank">Components - Edge Image Builder</a> or the Quickstart - Edge Image Builder guide (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p></div></section><section class="sect2" id="id-image-preparation" data-id-title="Image Preparation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.3 </span><span class="title-name">Image Preparation</span></span> <a title="Permalink" class="permalink" href="atip.html#id-image-preparation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Using <code class="literal">Edge Image Builder</code> to create the image for the management cluster, a lot of configurations can be customized, but in this document, we will cover the minimal configurations necessary to set up the management cluster.
Edge Image Builder is typically run from inside a container so, if you don’t already have a way to run containers, we need to start by installing a container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a>. For this guide, we will assume you already have a container runtime available.</p><section class="sect3" id="id-directory-structure" data-id-title="Directory Structure"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.1 </span><span class="title-name">Directory Structure</span></span> <a title="Permalink" class="permalink" href="atip.html#id-directory-structure">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When running the <code class="literal">EIB</code>, a directory will be mounted from the host, so the first thing to do is to create a directory structure to be used by the <code class="literal">EIB</code> to store the configuration files and the image itself.
This directory has the following structure:</p><div class="verbatim-wrap"><pre class="screen">├── mgmt-cluster.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
├── network/
|   └ mgmt-cluster-network.yaml
├── kubernetes/
|   └ config/
|   | └ server.yaml
└── custom/
    └ files/
    | └ clusterctl.yaml
    | └ helm-values-metal3.yaml
    └ scripts/
    | └ install-dependencies.sh</pre></div><div id="id-1.7.3.7.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The image <code class="literal">SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso</code> has to be downloaded from the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a> or the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Download page</a>, and it has to be located under the <code class="literal">base-images</code> folder.</p></div></section><section class="sect3" id="id-management-cluster-definition" data-id-title="Management Cluster definition"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.2 </span><span class="title-name">Management Cluster definition</span></span> <a title="Permalink" class="permalink" href="atip.html#id-management-cluster-definition">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">mgmt-cluster.yaml</code> file is the main configuration file for the management cluster. It contains the following information:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
    unattended: true
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}</pre></div><p>To explain the fields and values in the <code class="literal">mgmt-cluster.yaml</code> file, we have divided it into the following sections.</p></section><section class="sect3" id="id-image-section" data-id-title="Image section:"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.3 </span><span class="title-name">Image section:</span></span> <a title="Permalink" class="permalink" href="atip.html#id-image-section">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</pre></div><p>where the <code class="literal">baseImage</code> is the original image to be used which has been downloaded from the SUSE Customer Center or the SUSE Download page, and the <code class="literal">outputImageName</code> is the name of the image to be created which will be used to provision the management cluster.</p></section><section class="sect3" id="id-operating-system-section" data-id-title="Operating System section:"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.4 </span><span class="title-name">Operating System section:</span></span> <a title="Permalink" class="permalink" href="atip.html#id-operating-system-section">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
    unattended: true
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
  sccRegistrationCode: ${SCC_REGISTRATION_CODE}</pre></div><p>where the <code class="literal">installDevice</code> is the device to be used to install the operating system, the <code class="literal">unattended</code> is a flag to indicate if the installation will be unattended, the <code class="literal">username</code> and <code class="literal">encryptedPassword</code> are the credentials to be used to access the system, the <code class="literal">packageList</code> is the list of packages to be installed and the <code class="literal">sccRegistrationCode</code> is the registration code to be used to register the system that can be obtained from the SUSE Customer Center.</p><p>The encrypted password can be generated using the <code class="literal">openssl</code> command as follows:</p><div class="verbatim-wrap"><pre class="screen">openssl passwd -6 MyPassword!123</pre></div><p>This will output something similar to:</p><div class="verbatim-wrap"><pre class="screen">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</pre></div></section><section class="sect3" id="id-kubernetes-section" data-id-title="Kubernetes section:"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.5 </span><span class="title-name">Kubernetes section:</span></span> <a title="Permalink" class="permalink" href="atip.html#id-kubernetes-section">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">kubernetes:
  version: ${KUBERNETES_VERSION}</pre></div><p>where <code class="literal">version</code> is the version of Kubernetes to be installed. In our case, we are using a RKE2 cluster, so the version has to be minor than 1.28 to be compatible with <code class="literal">Rancher</code> (e.g <code class="literal">v1.27.10+rke2r1</code>).</p></section><section class="sect3" id="mgmt-cluster-helm-values" data-id-title="Custom files section:"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.6 </span><span class="title-name">Custom files section:</span></span> <a title="Permalink" class="permalink" href="atip.html#mgmt-cluster-helm-values">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">custom/files</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">helm-values-metal3.yaml</code>: contains the configuration params about the <code class="literal">Metal<sup>3</sup></code> Helm chart to be used.</p></li><li class="listitem"><p><code class="literal">clusterctl.yaml</code>: contains the configuration params about the <code class="literal">CAPI</code> Helm chart to be used.</p></li></ul></div><p>The following variables have to be replaced:</p><p><code class="literal">${MGMT_CLUSTER_IP}</code>: The IP address of the management cluster.</p><div id="metal3-media-server" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The Media Server is an optional feature included in metal3. In case you want to use your own media server (file server), you can disable the <code class="literal">enable_metal3_media_server</code> on the following manifest.
In case you want to use the metal3 media server you also specify the following variable:
<code class="literal">${MEDIA_VOLUME_PATH}</code>: The path to the media volume to be used by the <code class="literal">Metal<sup>3</sup></code> component (e.g. <code class="literal">/home/metal3/bmh-image-cache</code>)</p></div><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  ironicIP: ${MGMT_CLUSTER_IP}
  enable_vmedia_tls: false
  enable_metal3_media_server: true

metal3-media:
  service:
    type: NodePort
    port: 6280

metal3-ironic:
  global:
    predictableNicNames: "true"
  service:
    type: NodePort

metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</pre></div><p>The <code class="literal">clusterctl.yaml</code> file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">images:
  all:
    repository: registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse</pre></div></section><section class="sect3" id="id-custom-scripts-section" data-id-title="Custom Scripts section:"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.7 </span><span class="title-name">Custom Scripts section:</span></span> <a title="Permalink" class="permalink" href="atip.html#id-custom-scripts-section">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">custom/scripts</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">install-dependencies.sh</code> script contains the commands to install the necessary dependencies to be installed in the management cluster, like <code class="literal">Rancher</code>, <code class="literal">Metal<sup>3</sup></code>, <code class="literal">Cert-Manager</code>, etc…​:</p></li></ul></div><p>The following steps are executed by the <code class="literal">install-dependencies.sh</code> script:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the folder to server enable the media server for the <code class="literal">Metal<sup>3</sup></code> component.</p></li><li class="listitem"><p>Copy the <code class="literal">helm-values-metal3.yaml</code> file to the <code class="literal">Metal<sup>3</sup></code> folder.</p></li><li class="listitem"><p>Create the installer script to install the necessary tools, like clusterctl, helm for the management cluster.</p></li><li class="listitem"><p>Wait for the cluster to be available.</p></li><li class="listitem"><p>Install the <code class="literal">Cert-Manager</code> component.</p></li><li class="listitem"><p>Install the <code class="literal">Local-Path-Provisioner</code> component (for a single-node cluster).</p></li><li class="listitem"><p>Install the <code class="literal">Rancher Prime</code> component.</p></li><li class="listitem"><p>Install the <code class="literal">Metal<sup>3</sup></code> component.</p></li><li class="listitem"><p>Install the <code class="literal">CAPI</code> component.</p></li><li class="listitem"><p>Create the systemd service to run the installer script during the first boot.</p></li></ul></div><p>The <code class="literal">install-dependencies.sh</code> script is as follows:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

mount /usr/local || true
mount /home || true

## create folder to server httpd media server
mkdir -p /home/metal3/bmh-image-cache

## copy the metal3 yaml file to metal3 folder
cp ./helm-values-metal3.yaml ./clusterctl.yaml ./disable-embedded-capi.yaml /home/metal3/

## KUBECTL command var
export KUBECTL=/var/lib/rancher/rke2/bin/kubectl

# Create the installer script
cat &lt;&lt;- EOF &gt; /usr/local/bin/mgmt-cluster-installer.sh
#!/bin/bash
set -euo pipefail

## install clusterctl and helm
curl -Lk https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.0/clusterctl-linux-amd64 -o /usr/local/bin/clusterctl
chmod +x /usr/local/bin/clusterctl
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

## Wait for RKE2 cluster to be available
until [ -f /etc/rancher/rke2/rke2.yaml ]; do sleep 2; done
# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
# Wait for the node to be available, meaning the K8s API is available
while ! ${KUBECTL} wait --for condition=ready node $(cat /etc/hostname | tr '[:upper:]' '[:lower:]') ; do sleep 2 ; done

## Add Helm repos
helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo add jetstack https://charts.jetstack.io
helm repo update

while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller ; do sleep 2 ; done

## install cert-manager
helm install cert-manager jetstack/cert-manager \
	--namespace cert-manager \
        --create-namespace \
        --set installCRDs=true \
	--version v1.11.1

## Local path provisioner
${KUBECTL} apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
until [ \$(${KUBECTL} get sc -o name | wc -l) -ge 1 ]; do sleep 10; done
${KUBECTL} patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

## Example in case you want to configure the httpd cache server for images
## podman run -dit --name bmh-image-cache -p 8080:80 -v /home/metal3/bmh-image-cache:/usr/local/apache2/htdocs/ docker.io/library/httpd:2.4

## install rancher
helm install rancher rancher-prime/rancher \
	--namespace cattle-system \
	--create-namespace \
	--set hostname=rancher-$(hostname -I | awk '{print $1}').sslip.io \
	--set bootstrapPassword=admin \
	--set replicas=1 \
        --set global.cattle.psp.enabled=false
while ! ${KUBECTL} wait --for condition=ready -n cattle-system \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name) --timeout=10s; do sleep 2 ; done

## install metal3 with helm
helm repo add suse-edge https://suse-edge.github.io/charts
helm install   metal3 suse-edge/metal3   --namespace metal3-system   --create-namespace -f /home/metal3/helm-values-metal3.yaml


## install capi
if [ \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name | wc -l) -ge 1 ]; then
	${KUBECTL} apply -f /home/metal3/disable-embedded-capi.yaml
	${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
	${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
	${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi
clusterctl init --core "cluster-api:v1.6.0" --infrastructure "metal3:v1.6.0" --bootstrap "rke2:v0.2.6" --control-plane "rke2:v0.2.6" --config /home/metal3/clusterctl.yaml

rm -f /etc/systemd/system/mgmt-cluster-installer.service
EOF

chmod a+x /usr/local/bin/mgmt-cluster-installer.sh

cat &lt;&lt;- EOF &gt; /etc/systemd/system/mgmt-cluster-installer.service
[Unit]
Description=Deploy mgmt cluster tools on K3S/RKE2
Wants=network-online.target
After=network.target network-online.target rke2-server.target
ConditionPathExists=/usr/local/bin/mgmt-cluster-installer.sh

[Service]
User=root
Type=forking
TimeoutStartSec=900
ExecStart=/usr/local/bin/mgmt-cluster-installer.sh
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /usr/local/bin/mgmt-cluster-installer.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-cluster-installer.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-cluster-installer.service

[Install]
WantedBy=multi-user.target
EOF

systemctl enable mgmt-cluster-installer.service

umount /usr/local || true
umount /home || true</pre></div></section><section class="sect3" id="id-kubernetes-definition-optional" data-id-title="Kubernetes definition (optional)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.8 </span><span class="title-name">Kubernetes definition (optional)</span></span> <a title="Permalink" class="permalink" href="atip.html#id-kubernetes-definition-optional">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>By default, the <code class="literal">CNI</code> plugin installed by default is <code class="literal">Cilium</code>, so you don’t need to create this file. Just in case you need to customize the <code class="literal">CNI</code> plugin, you can use the <code class="literal">server.yaml</code> file under the <code class="literal">kubernetes/config</code> folder. It contains the following information:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- cilium</pre></div><p>This is an optional file to define some Kubernetes customization like the CNI plugins to be used or many options you can check in the <a class="link" href="https://docs.rke2.io/install/configuration" target="_blank">official documentation</a>.</p></section><section class="sect3" id="id-networking-definition-optional" data-id-title="Networking definition (optional)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.3.3.9 </span><span class="title-name">Networking definition (optional)</span></span> <a title="Permalink" class="permalink" href="atip.html#id-networking-definition-optional">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>In case you need to customize the networking configuration, for example, in case you need to use a specific IP address (DHCP-less scenario), you can use the <code class="literal">mgmt-cluster-network.yaml</code> file under the <code class="literal">network</code> folder. It contains the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${MGMT_GATEWAY}</code>: The gateway IP address.</p></li><li class="listitem"><p><code class="literal">${MGMT_DNS}</code>: The DNS server IP address.</p></li><li class="listitem"><p><code class="literal">${MGMT_MAC}</code>: The MAC address of the network interface.</p></li><li class="listitem"><p><code class="literal">${MGMT_CLUSTER_IP}</code>: The IP address of the management cluster.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</pre></div></section></section><section class="sect2" id="id-image-creation" data-id-title="Image Creation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.4 </span><span class="title-name">Image Creation</span></span> <a title="Permalink" class="permalink" href="atip.html#id-image-creation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD/eib/:/eib \
 registry.opensuse.org/isv/suse/edge/edgeimagebuilder/containerfile/suse/edge-image-builder:1.0.0 \
 --config-file mgmt-cluster.yaml --config-dir /eib --build-dir /eib/_build</pre></div><p>This will create the iso output image file that in our case based on the image definition described above will be <code class="literal">eib-mgmt-cluster-image.iso</code>.
This image contains all components inside, and it can be used to provision the management cluster using a virtual machine, a bare metal server (using the virtual-media feature)</p></section></section><section class="sect1" id="id-telco-features-configuration" data-id-title="Telco Features Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.4 </span><span class="title-name">Telco Features Configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#id-telco-features-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section documents the explanation and how to configure Telco specific features on ATIP deployed clusters.</p><p>The configuration and the provision based on a <code class="literal">Zero Touch Provisioning</code> (ZTP) process will be described in the <a class="link" href="atip-automated-provision.xml" target="_blank">ATIP Automated Provision</a> section.</p><p>The following topics will be covered in this section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel image for Real Time (<a class="xref" href="atip.html#kernel-image-for-real-time" title="21.4.1. Kernel Image for Real Time">Section 21.4.1, “Kernel Image for Real Time”</a>): Kernel image to be used by the real time kernel.</p></li><li class="listitem"><p>CPU Tuned configuration (<a class="xref" href="atip.html#cpu-tuned-configuration" title="21.4.2. CPU Tuned Configuration">Section 21.4.2, “CPU Tuned Configuration”</a>): Tuned configuration to be used by the real time kernel.</p></li><li class="listitem"><p>CNI configuration (<a class="xref" href="atip.html#cni-configuration" title="21.4.3. CNI Configuration">Section 21.4.3, “CNI Configuration”</a>): CNI configuration to be used by the Kubernetes cluster.</p></li><li class="listitem"><p>SR-IOV configuration (<a class="xref" href="atip.html#sriov" title="21.4.4. SR-IOV">Section 21.4.4, “SR-IOV”</a>): SR-IOV configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>DPDK configuration (<a class="xref" href="atip.html#dpdk" title="21.4.5. DPDK">Section 21.4.5, “DPDK”</a>): DPDK configuration to be used by system.</p></li><li class="listitem"><p>vRAN Acceleration card (<a class="xref" href="atip.html#acceleration" title="21.4.6. vRAN Acceleration (Intel ACC100/ACC200)">Section 21.4.6, “vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)”</a>): Acceleration card configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Huge Pages (<a class="xref" href="atip.html#huge-pages" title="21.4.7. Huge Pages">Section 21.4.7, “Huge Pages”</a>): Huge Pages configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>CPU Pinning configuration (<a class="xref" href="atip.html#cpu-pinning-configuration" title="21.4.8. CPU Pinning Configuration">Section 21.4.8, “CPU Pinning Configuration”</a>): CPU Pinning configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>NUMA Aware scheduling configuration (<a class="xref" href="atip.html#numa-aware-scheduling" title="21.4.9. NUMA Aware scheduling">Section 21.4.9, “NUMA Aware scheduling”</a>): NUMA Aware scheduling configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Metal LB configuration (<a class="xref" href="atip.html#metal-lb-configuration" title="21.4.10. Metal LB">Section 21.4.10, “Metal LB”</a>): Metal LB configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Private Registry configuration (<a class="xref" href="atip.html#private-registry" title="21.4.11. Private Registry Configuration">Section 21.4.11, “Private Registry Configuration”</a>): Private Registry configuration to be used by the Kubernetes workloads.</p></li></ul></div><section class="sect2" id="kernel-image-for-real-time" data-id-title="Kernel Image for Real Time"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.1 </span><span class="title-name">Kernel Image for Real Time</span></span> <a title="Permalink" class="permalink" href="atip.html#kernel-image-for-real-time">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The real time kernel image is not necessarily better than a standard kernel.
It is a different kernel tuned to a specific use case. The real time kernel is tuned for lower latency at the cost of throughput. The real time kernel is not recommended for general purpose use, but in our case, this is the recommended kernel for Telco Workloads where latency is a key factor.</p><p>There are 4 top features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Deterministic Execution:</p><p>Get greater predictability — ensure critical business processes complete in time, every time and deliver high quality of service, even under heavy system loads. By shielding key system resources for high-priority processes, you can ensure greater predictability for time-sensitive applications.</p></li><li class="listitem"><p>Low Jitter:</p><p>The low jitter built upon the highly deterministic technology helps to keep applications synchronized with the real world. This helps services that need ongoing and repeated calculation.</p></li><li class="listitem"><p>Priority Inheritance:</p><p>Priority inheritance refers to the ability of a lower priority process to assume a higher priority when there is a higher priority process that requires the lower priority process to finish before it can accomplish its task. SUSE Linux Enterprise Real Time solves these priority inversion problems for mission-critical processes.</p></li><li class="listitem"><p>Thread Interrupts:</p><p>Processes running in interrupt mode in a general-purpose operating system are not preemptible. With SUSE Linux Enterprise Real Time these interrupts have been encapsulated by kernel threads, which are interruptible, and in turn allow the hard and soft interrupts to be preempted by user-defined higher priority processes.</p><p>In our case, if you have installed a real time image like <code class="literal">SLE Micro RT</code>, kernel real time is already installed. From the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, you can download the real time kernel image.</p><div id="id-1.7.3.8.6.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the real time kernel, please visit <a class="link" href="https://www.suse.com/products/realtime/" target="_blank">SUSE Real Time</a></p></div></li></ul></div></section><section class="sect2" id="cpu-tuned-configuration" data-id-title="CPU Tuned Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.2 </span><span class="title-name">CPU Tuned Configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#cpu-tuned-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The CPU Tuned configuration allows the possibility to isolate the CPU cores to be used by the real time kernel. It’s really important to avoid the OS to use the same cores as the real time kernel, because the OS could use the cores and increase the latency in the real time kernel.</p><p>To enable and configure this feature, the first thing is to create a profile for the cpu cores we want to isolate. In this case, we will isolate the cores <code class="literal">1-30</code> and <code class="literal">33-62</code>.</p><div class="verbatim-wrap"><pre class="screen">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</pre></div><p>Then we need to modify grub option to isolate cpu cores as well as another important parameters for the cpu usage.
The following options are important to be customized with your current hardware specifications:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-30 and 33-62</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the timer tick on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>kthread_cpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the kthreads on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the interrupts on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>processor.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option prevents the CPU from dropping into a sleep state when idle</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_idle.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; "><p>This option disables the intel_idle driver and allows acpi_idle to be used</p></td></tr></tbody></table></div><p>With the values showed above, we are isolating 60 cores, and we are using 4 cores for the OS.</p><p>The following commands will modify the grub configuration and apply the changes mentioned above to be present on the next boot:</p><p>Edit the <code class="literal">/etc/default/grub</code> file and add the parameters mentioned above:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</pre></div><p>Update the grub configuration:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after reboot, the following command can be used to check the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div></section><section class="sect2" id="cni-configuration" data-id-title="CNI Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.3 </span><span class="title-name">CNI Configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#cni-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect3" id="id-cilium" data-id-title="Cilium"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.4.3.1 </span><span class="title-name">Cilium</span></span> <a title="Permalink" class="permalink" href="atip.html#id-cilium">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Cilium</code> is the default CNI plugin for ATIP.
To enable Cilium on RKE2 cluster as the default plugin the following configurations are required in the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- cilium</pre></div><p>This can also be specified with command-line arguments, i.e. <code class="literal">--cni=cilium</code>  into the server line in <code class="literal">/etc/systemd/system/rke2-server</code> file.</p><p>If you want to use the <code class="literal">SR-IOV</code> network operator described in the next section (<a class="xref" href="atip.html#option2-sriov-helm">Section 21.4.4, “SR-IOV”</a>), you need to use <code class="literal">Multus</code> with another CNI plugin like <code class="literal">Cilium</code> or <code class="literal">Calico</code> as a secondary plugin.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- cilium</pre></div><div id="id-1.7.3.8.8.2.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about CNI plugins, please visit <a class="link" href="https://docs.rke2.io/install/network_options" target="_blank">Network Options</a></p></div></section></section><section class="sect2" id="sriov" data-id-title="SR-IOV"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.4 </span><span class="title-name">SR-IOV</span></span> <a title="Permalink" class="permalink" href="atip.html#sriov">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>SR-IOV allows a device, such as a network adapter, to separate access to its resources among various <code class="literal">PCIe</code> hardware functions.
There are different ways to deploy <code class="literal">SR-IOV</code>, and in this case, we will show two different options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Option 1: using the <code class="literal">SR-IOV</code> CNI device plugins and a config map to configure it properly.</p></li><li class="listitem"><p>Option 2 (recommended): using the <code class="literal">SR-IOV</code> helm chart from Rancher Prime to make this deployment easy.</p></li></ul></div><p id="option1-sriov-deviceplugin"><span class="strong"><strong>Option 1 - Installation of SR-IOV CNI device plugins and a config map to configure it properly</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the config map for the device plugin</p></li></ul></div><p>You could get the information to fill the config map from the <code class="literal">lspci</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</pre></div><p>The config map consists of a <code class="literal">JSON</code> file that describe devices using filters to discover and creates some groups for the interfaces.
The most important is to understand the filters and the groups. The filters are used to discover the devices and the groups are used to create the interfaces.</p><p>It could be possible set some filters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>vendorID: <code class="literal">8086</code> (Intel)</p></li><li class="listitem"><p>deviceID: <code class="literal">0d5c</code> (Accelerator card)</p></li><li class="listitem"><p>driver: <code class="literal">vfio-pci</code> (driver)</p></li><li class="listitem"><p>pfNames: <code class="literal">p2p1</code> (physical interface name)</p></li></ul></div><p>It could be possible also set filters to match more complex interfaces syntax like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>pfNames: <code class="literal">["eth1#1,2,3,4,5,6"]</code> or <code class="literal">[eth1#1-6]</code> (physical interface name)</p></li></ul></div><p>Related to the groups, we could create a group for the <code class="literal">FEC</code> card and another group for the <code class="literal">Intel</code> card even creating some prefix depending on our use case:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>resourceName: <code class="literal">pci_sriov_net_bh_dpdk</code></p></li><li class="listitem"><p>resourcePrefix: <code class="literal">Rancher.io</code></p></li></ul></div><p>There are a lot of combinations in order to discover and create the resource group to allocate some <code class="literal">VFs</code> to the pods.</p><div id="id-1.7.3.8.9.16" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the filters and groups, please visit <a class="link" href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank">sr-iov network device plugin</a></p></div><p>After setting the filters and groups to match the interfaces depending on the hardware and the use case, the following config map shows an example to be used:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the <code class="literal">daemonset</code> file to deploy the device plugin</p></li></ul></div><p>No changes are needed in the <code class="literal">daemonset</code>, so you can use the same <a class="link" href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin/blob/master/deployments/sriovdp-daemonset.yaml" target="_blank">upstream</a> <code class="literal">daemonset</code> file.</p><p>The device plugin support several architectures (<code class="literal">arm</code>, <code class="literal">amd</code>, <code class="literal">ppc64le</code>), so the same file can be used for different architectures deploying several <code class="literal">daemonset</code> for each architecture.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.5.1-build20231009-amd64
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>After applying the config map and the <code class="literal">daemonset</code>, the device plugin will be deployed and the interfaces will be discovered and available for the pods.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</pre></div></li><li class="listitem"><p>Check the interfaces discovered and available in the nodes to be used by the pods:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</pre></div></li><li class="listitem"><p>The <code class="literal">FEC</code> will be <code class="literal">intel.com/intel_fec_5g</code> and the value will be 1</p></li><li class="listitem"><p>The <code class="literal">VF</code> will be <code class="literal">intel.com/intel_sriov_odu</code> or <code class="literal">intel.com/intel_sriov_oru</code> if you deploy it with device plugin and the config map without helm charts</p></li></ul></div><div id="id-1.7.3.8.9.24" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>If there is no interfaces available here, does not make sense continue because interface will not be available for pods. Review the config map and filters to solve the issue first.</p></div><p id="option2-sriov-helm"><span class="strong"><strong>Option 2 (Recommended) - Installation using Rancher using Helm chart for SR-IOV CNI and device plugins</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Get helm if not present</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install SR-IOV</p></li></ul></div><p>This part could be done in two ways, using the <code class="literal">CLI</code> or using the <code class="literal">Rancher UI</code></p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.3.8.9.30.1"><span class="term">Install Operator from CLI</span></dt><dd><div class="verbatim-wrap"><pre class="screen">helm repo add rancher-charts suse-edge https://suse-edge.github.io/charts
helm install sriov-crd suse-edge/sriov-crd
helm install install sriov-network-operator suse-edge/sriov-network-operator</pre></div></dd><dt id="id-1.7.3.8.9.30.2"><span class="term">Install Operator from Rancher UI</span></dt><dd><p>Once your cluster is installed, and you have access to the <code class="literal">Rancher UI</code>, you can install the <code class="literal">SR-IOV Operator</code> from the <code class="literal">Rancher UI</code> from the apps tab:</p><div class="informalfigure"><div class="mediaobject"><a href="images/features_sriov.png"><img src="images/features_sriov.png" width="NaN" alt="sriov.png" title="sriov.png"/></a></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the  deployed resources crd and pods</p></li></ul></div></dd></dl></div><div class="verbatim-wrap"><pre class="screen">$ kubectl  get crd
$ kubectl -n kube-system get pods</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the label in the nodes</p></li></ul></div><p>With all resources running, the label will appear automatically in your node:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the <code class="literal">daemonset</code> to see the new <code class="literal">sriov-network-config-daemon</code> and <code class="literal">sriov-rancher-nfd-worker</code> as active and ready</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system         calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
cattle-sriov-system   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
cattle-sriov-system   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system           rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system           rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</pre></div><p>After some minutes (can take up to 10 min to be updated), the nodes are detected and configured with the <code class="literal">SR-IOV</code> capabilities:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
cattle-sriov-system   xr11-2   83s</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the interfaces detected</p></li></ul></div><p>The interfaces discovered should be the pci address of the network device. Check this information with <code class="literal">lspci</code> command in the host.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: cattle-sriov-system
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</pre></div><div id="id-1.7.3.8.9.42" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>If your interface is not detected here you should ensure that it is present in the next config map</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get cm supported-nic-ids -oyaml -n cattle-sriov-system</pre></div><p>If your device is not there you have to edit the config map adding the right values to be discovered (should be necessary to restart the <code class="literal">sriov-network-config-daemon</code> daemonset)</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">NetworkNode Policy</code> to configure the <code class="literal">VFs</code></p></li></ul></div><p>Basically, some <code class="literal">VFs</code> (<code class="literal">numVfs</code>) from the device (<code class="literal">rootDevices</code>) wil be created, and it will be configured with the driver <code class="literal">deviceType</code> and the <code class="literal">MTU</code>:</p><div id="id-1.7.3.8.9.45" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">resourceName</code> field should not contain any special characters, and it should be unique across the cluster.</p></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: kube-system
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Validate configurations</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the sr-iov network (optional, just in case a different network is needed):</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: kube-system
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the network created:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: kube-system
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.3.1", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</pre></div></section><section class="sect2" id="dpdk" data-id-title="DPDK"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.5 </span><span class="title-name">DPDK</span></span> <a title="Permalink" class="permalink" href="atip.html#dpdk">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">DPDK</code> (Data Plane Development Kit) is a set of libraries and drivers for fast packet processing. It is used to accelerate packet processing workloads running on a wide variety of CPU architectures.
The DPDK includes data plane libraries and optimized network interface controller (<code class="literal">NIC</code>) drivers for the folowing:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A queue manager implements lockless queues.</p></li><li class="listitem"><p>A buffer manager pre-allocates fixed size buffers.</p></li><li class="listitem"><p>A memory manager allocates pools of objects in memory and uses a ring to store free objects; ensures that objects are spread equally on all <code class="literal">DRAM</code> channels.</p></li><li class="listitem"><p>Poll mode drivers (<code class="literal">PMD</code>) are designed to work without asynchronous notifications, reducing overhead.</p></li><li class="listitem"><p>A packet framework as a set of libraries that are helpers to develop packet processing.</p></li></ol></div><p>The following steps will show how to enable <code class="literal">DPDK</code> and how to create <code class="literal">VFs</code> from the <code class="literal">NICs</code> to be used by the <code class="literal">DPDK</code> interfaces:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install the <code class="literal">DPDK</code> package</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ transactional-update pkg install dpdk22 dpdk22-tools libdpdk-23
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To use dpdk using some drivers is required to enable some parameters in the kernel:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows to use <code class="literal">vfio</code> driver for the dpdk interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables to use <code class="literal">vfio</code> for <code class="literal">VFs</code>.</p></td></tr></tbody></table></div><p>To enable the parameters is required to add them to the <code class="literal">/etc/default/grub</code> file:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</pre></div><p>Update the grub configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load <code class="literal">vfio-pci</code> kernel module and enable <code class="literal">SR-IOV</code> on the <code class="literal">NICs</code>:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create some virtual functions (<code class="literal">VFs</code>) from the <code class="literal">NICs</code></p></li></ul></div><p>To create 4 <code class="literal">VFs</code>, for example, for 2 different <code class="literal">NICs</code> the following commands are required:</p><div class="verbatim-wrap"><pre class="screen">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the new VFs with the <code class="literal">vfio-pci</code> driver</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the configuration is correctly applied:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</pre></div></section><section class="sect2" id="acceleration" data-id-title="vRAN Acceleration (Intel ACC100/ACC200)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.6 </span><span class="title-name">vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)</span></span> <a title="Permalink" class="permalink" href="atip.html#acceleration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>As communications service providers move from 4G to 5G networks, many are adopting virtualized radio access network (<code class="literal">vRAN</code>) architectures for higher channel capacity and easier deployment of edge-based services and applications. vRAN solutions are ideally located to deliver low-latency services with the flexibility to increase or decrease capacity based on the volume of real-time traffic and demand on the network.</p><p>One of the most compute-intensive 4G and 5G workloads is RAN layer 1 (<code class="literal">L1</code>) <code class="literal">FEC</code>, which resolves data transmission errors over unreliable or noisy communication channels. <code class="literal">FEC</code> technology detects and corrects a limited number of errors in 4G or 5G data, eliminating the need for retransmission. Since the <code class="literal">FEC</code> acceleration transaction does not contain cell state information, it can be easily virtualized, enabling pooling benefits and easy cell migration.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To enable the <code class="literal">vRAN</code> acceleration we need to enable the following kernel parameters (if not present yet):</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows to use vfio for the dpdk interfaces</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables to use vfio for VFs.</p></td></tr></tbody></table></div><p>Modify the grub file <code class="literal">/etc/default/grub</code> to add them to the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</pre></div><p>Update the grub configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot you can check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load vfio-pci kernel modules to enable the <code class="literal">vRAN</code> acceleration</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Get interface information Acc100</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the physical interface (<code class="literal">PF</code>) with <code class="literal">vfio-pci</code> driver</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the virtual functions (<code class="literal">VFs</code>) from the physical interface (<code class="literal">PF</code>)</p></li></ul></div><p>Create 2 <code class="literal">VFs</code> from the <code class="literal">PF</code> and bind with <code class="literal">vfio-pci</code> following the next steps:</p><div class="verbatim-wrap"><pre class="screen">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Configure acc100 with the proposed configuration file</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the new VFs created from the FEC PF:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</pre></div></section><section class="sect2" id="huge-pages" data-id-title="Huge Pages"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.7 </span><span class="title-name">Huge Pages</span></span> <a title="Permalink" class="permalink" href="atip.html#huge-pages">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When a process uses <code class="literal">RAM</code>, the <code class="literal">CPU</code> marks it as used by that process. For efficiency, the <code class="literal">CPU</code> allocates <code class="literal">RAM</code> in chunks <code class="literal">4K</code> bytes is the default value on many platforms. Those chunks are named pages. Pages can be swapped to disk, etc.</p><p>Since the process address space is virtual, the <code class="literal">CPU</code> and the operating system need to remember which pages belong to which process, and where each page is stored. The more pages you have, the more time it takes to find where memory is mapped. When a process uses <code class="literal">1GB</code> of memory, that’s 262144 entries to look up (<code class="literal">1GB</code> / <code class="literal">4K</code>). If a page table entry consume 8 bytes, that’s <code class="literal">2MB</code> (262144 * 8) to look up.</p><p>Most current <code class="literal">CPU</code> architectures support larger-than-default pages, which give the <code class="literal">CPU/OS</code> less entries to look-up.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To enable the huge pages we should add the next kernel parameters:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This options allows to set the size of huge pages to 1G</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This is the number of hugepages defined before</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; "><p>This is the default value to get the huge pages</p></td></tr></tbody></table></div><p>Modify the grub file <code class="literal">/etc/default/grub</code> to add them to the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</pre></div><p>Update the grub configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot you can check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Usage of huge pages</p></li></ul></div><p>To use the huge pages we need to mount them:</p><div class="verbatim-wrap"><pre class="screen">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</pre></div><p>Deploy a Kubernetes workload creating the resources as well as the volumes:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</pre></div><div class="verbatim-wrap highlight yaml"><pre class="screen">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</pre></div></section><section class="sect2" id="cpu-pinning-configuration" data-id-title="CPU Pinning Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.8 </span><span class="title-name">CPU Pinning Configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#cpu-pinning-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Requirements</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Must have the <code class="literal">CPU</code> tuned to the performance profile covered on this section (<a class="xref" href="atip.html#cpu-tuned-configuration" title="21.4.2. CPU Tuned Configuration">Section 21.4.2, “CPU Tuned Configuration”</a>)</p></li><li class="listitem"><p>Must have the <code class="literal">RKE2</code> cluster kubelet configured with the cpu management arguments adding the following block (as an example) to the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p></li></ol></div></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">kubelet-arg:
- "cpu-manager=true"
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=1"
- "system-reserved=cpu=1"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Use CPU Pinning on Kubernetes</p></li></ul></div><p>There are three ways to use that feature using the <code class="literal">Static Policy</code> defined in kubelet depending on the requests and limits you define on your workload:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">BestEffort</code> QoS Class: If you don’t define any request or limit for <code class="literal">CPU</code>, the pod will be scheduled on the first <code class="literal">CPU</code> available on the system.</p><p>An example to use the <code class="literal">BestEffort</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx</pre></div></li><li class="listitem"><p><code class="literal">Burstable</code> QoS Class: If you define a request for CPU, which is not equal to the limits, or maybe there is no CPU request.</p><p>Examples to use the <code class="literal">Burstable</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</pre></div><p>or</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</pre></div></li><li class="listitem"><p><code class="literal">Guaranteed</code> QoS Class: If you define a request for CPU, which is equal to the limits.</p><p>An example to use the <code class="literal">Guaranteed</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</pre></div></li></ol></div></section><section class="sect2" id="numa-aware-scheduling" data-id-title="NUMA Aware scheduling"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.9 </span><span class="title-name">NUMA Aware scheduling</span></span> <a title="Permalink" class="permalink" href="atip.html#numa-aware-scheduling">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Non-Uniform Memory Access or Non-Uniform Memory Architecture (<code class="literal">NUMA</code>) is a physical memory design used in <code class="literal">SMP</code> (multiprocessors) architecture, where the memory access time depends on the memory location relative to a processor. Under <code class="literal">NUMA</code>, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors.</p><section class="sect3" id="id-identify-numa-nodes" data-id-title="Identify NUMA nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.4.9.1 </span><span class="title-name">Identify NUMA nodes</span></span> <a title="Permalink" class="permalink" href="atip.html#id-identify-numa-nodes">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To identify the <code class="literal">NUMA</code> nodes on your system use the following command:</p><div class="verbatim-wrap"><pre class="screen">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</pre></div><div id="id-1.7.3.8.14.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For this example, we have only one <code class="literal">NUMA</code> node showing 64 <code class="literal">CPUs</code>.</p><p><code class="literal">NUMA</code> has to enabled in the <code class="literal">BIOS</code>. If <code class="literal">dmesg</code> does not have records of numa initialization during bootup, then it is possible that <code class="literal">NUMA</code> related messages in the kernel ring buffer might have been overwritten.</p></div></section></section><section class="sect2" id="metal-lb-configuration" data-id-title="Metal LB"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.10 </span><span class="title-name">Metal LB</span></span> <a title="Permalink" class="permalink" href="atip.html#metal-lb-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">MetalLB</code> is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols like <code class="literal">L2</code> and <code class="literal">BGP</code> as a advertisement protocols. It is a network load balancer that can be used to expose services in a Kubernetes cluster to the outside world due to the need to use Kubernetes Services type <code class="literal">LoadBalancer</code> with bare metal.</p><p>To enable <code class="literal">MetalLB</code> in the <code class="literal">RKE2</code> cluster, the following steps are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install <code class="literal">MetalLB</code> using the following command:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: default
spec:
  repo: https://metallb.github.io/metallb/
  chart: metallb
  targetNamespace: default
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: default
spec:
  repo: https://suse-edge.github.io/endpoint-copier-operator
  chart: endpoint-copier-operator
  targetNamespace: default
EOF</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">IpAddressPool</code> and the <code class="literal">L2advertisement</code> configuration:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: default
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: default
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the endpoint service to expose the <code class="literal">VIP</code>:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the <code class="literal">VIP</code> is created and the <code class="literal">MetalLB</code> pods are running:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get svc -n default
$ kubectl get pods -n default</pre></div></section><section class="sect2" id="private-registry" data-id-title="Private Registry Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.4.11 </span><span class="title-name">Private Registry Configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#private-registry">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Containerd</code> can be configured to connect to private registries and use them to pull private images on each node.</p><p>Upon startup, <code class="literal">RKE2</code> will check to see if a <code class="literal">registries.yaml</code> file exists at <code class="literal">/etc/rancher/rke2/</code> and instruct <code class="literal">containerd</code> to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.</p><p>To add the private registry, create the file <code class="literal">/etc/rancher/rke2/registries.yaml</code> with the following content:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
docker.io:
endpoint:
- "https://registry.example.com:5000"
configs:
"registry.example.com:5000":
auth:
username: xxxxxx # this is the registry username
password: xxxxxx # this is the registry password
tls:
cert_file:            # path to the cert file used to authenticate to the registry
key_file:             # path to the key file for the certificate used to authenticate to the registry
ca_file:              # path to the ca file used to verify the registry's certificate
insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>or without authentication:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>In order for the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.</p><div id="id-1.7.3.8.16.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about this, please check <a class="link" href="https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file" target="_blank">containerd registry configuration rke2</a>.</p></div></section></section><section class="sect1" id="id-automated-provisioning-with-ztp" data-id-title="Automated Provisioning with ZTP"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.5 </span><span class="title-name">Automated Provisioning with ZTP</span></span> <a title="Permalink" class="permalink" href="atip.html#id-automated-provisioning-with-ztp">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-introduction-2" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="atip.html#id-introduction-2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The downstream cluster provisioning with <code class="literal">ZTP</code> (Zero Touch Provisioning) is a feature that allows you to automate the provisioning of downstream clusters. This feature is useful when you have a large number of downstream clusters to provision, and you want to automate the process.
From a technical point of view, the management cluster contains the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">SUSE Linux Enterprise Micro RT</code> as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.</p></li><li class="listitem"><p><code class="literal">RKE2</code> as the Kubernetes cluster. The default <code class="literal">CNI</code> plugin is <code class="literal">Cilium</code>.  Depending on the use case, some <code class="literal">CNI</code> plugins can be used, such as <code class="literal">Cilium+Multus</code>.</p></li><li class="listitem"><p><code class="literal">Longhorn</code> as the storage solution.</p></li><li class="listitem"><p><code class="literal">NeuVector</code> as the security solution.</p></li></ul></div><p>Depending on the use case, <code class="literal">MetalLB</code> can be used as the load balancer for the Kubernetes cluster to enable high availability on multi-node clusters.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">MetalLB</code> as the component to deploy multi-node clusters using a load balancer.</p></li></ul></div><div id="id-1.7.3.9.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">SUSE Linux Enterprise Micro</code>, please check: Components - SUSE Linux Enterprise Micro (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">RKE2</code>, please check: Components - RKE2 (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">Longhorn</code>, please check: Components - Longhorn (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p><p>For more information about <code class="literal">NeuVector</code>, please check: Components - NeuVector (<span class="intraxref">Book “SUSE Edge Documentation”</span>)</p></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provisioning of the downstream clusters. The following sections describe the different <code class="literal">ZTP</code> workflows and some additional features that can be added to the provisioning process:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a></p></li><li class="listitem"><p><a class="xref" href="atip.html#ztp-single-node" title="21.5.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 21.5.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip.html#ztp-multi-node" title="21.5.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 21.5.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip.html#ztp-add-advanced-network" title="21.5.5. Additional Features with ZTP - advanced network configuration">Section 21.5.5, “Additional Features with ZTP - advanced network configuration”</a></p></li><li class="listitem"><p><a class="xref" href="atip.html#ztp-add-telco" title="21.5.6. Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)">Section 21.5.6, “Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)”</a></p></li><li class="listitem"><p><a class="xref" href="atip.html#ztp-private-registry" title="21.5.7. Additional Features with ZTP - Private Registry">Section 21.5.7, “Additional Features with ZTP - Private Registry”</a></p></li></ul></div></section><section class="sect2" id="ztp-eib-edge-image" data-id-title="Generate the downstream Image using EIB"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.2 </span><span class="title-name">Generate the downstream Image using EIB</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-eib-edge-image">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Using <code class="literal">Edge Image Builder</code> to create the image to be used in the downstream clusters, a lot of configurations can be customized, but in this guide, we will cover the minimal configurations necessary to set up the downstream cluster.
Edge Image Builder is typically run from inside a container so, if you don’t already have a way to run containers, we need to start by installing a container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a>. For this document, we will assume you already have a container runtime available.</p><p><span class="strong"><strong>Directory Structure</strong></span></p><p>When running the <code class="literal">EIB</code>, a directory will be mounted from the host, so the first thing to do is to create a directory structure to be used by the <code class="literal">EIB</code> to store the configuration files and the image itself.
This directory has the following structure:</p><div class="verbatim-wrap"><pre class="screen">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ growfs.sh</pre></div><p>Where the <code class="literal">network</code> folder is optional, just in case you need to customize for DHCP-less, or advanced networking scenarios (covered in the following sections).</p><div id="id-1.7.3.9.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The image <code class="literal">SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</code> has to be downloaded from the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a> or the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Download page</a>, and it has to be located under the <code class="literal">base-images</code> folder.</p></div><p><span class="strong"><strong>Downstream Cluster config file</strong></span></p><p>The <code class="literal">downstream-cluster-config.yaml</code> file is the main configuration file for the downstream cluster.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Depending on the use case, the common configuration file for all scenarios contains the following information:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</pre></div><p>where the <code class="literal">${ROOT_PASSWORD}</code> is the encrypted password for the root user. Just for testing purposes, if you want to generate the encrypted password, you can use the following command:</p><div class="verbatim-wrap"><pre class="screen">openssl passwd -6 PASSWORD</pre></div><p>For production environment, the recommendation is to use the ssh-keys which can be added to the users block replacing the <code class="literal">${USERKEY1}</code> with the real ssh keys.</p><div class="itemizedlist" id="ztp-add-telco-feature-eib"><ul class="itemizedlist"><li class="listitem"><p>In case you need to configure some Telco features like <code class="literal">dpdk</code>, <code class="literal">sr-iov</code> or <code class="literal">FEC</code>, the following file includes the packages required to enable these features:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${user1Key1}
  packages:
    packageList:
      - jq
      - dpdk22
      - dpdk22-tools
      - libdpdk-23
      - pf-bb-config
    additionalRepos:
      - url: https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SLEMicro5.5/
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</pre></div><p>Where the <code class="literal">${SCC_REGISTRATION_CODE}</code> is the registration code for the SUSE Customer Center, and the package list contains the minimum packages to be used for the Telco profiles.
In case you want to use the <code class="literal">pf-bb-config</code> package (to enable the <code class="literal">FEC</code> feature and binding with drivers), the <code class="literal">additionalRepos</code> block has to be included to add the <code class="literal">SUSE Edge Telco</code> repository.</p><div class="itemizedlist" id="ztp-add-network-eib"><ul class="itemizedlist"><li class="listitem"><p>In case you need to configure the networking for a DHCP-less or more networking advanced scenarios, the following folder and file should be included with the networking configuration:</p></li></ul></div><p>In the <code class="literal">network</code> folder, the <code class="literal">configure-network.sh</code> file contains the networking configuration for the downstream cluster.
The following script tries to statically configure a NIC when the bare metal object has been created with a secret containing the static network configuration covered in the Additional Features with ZTP - DHCP Less (<a class="xref" href="atip.html#ztp-add-advanced-network" title="21.5.5. Additional Features with ZTP - advanced network configuration">Section 21.5.5, “Additional Features with ZTP - advanced network configuration”</a>) section.
Also, it uses the networking information to generate the network configuration for the downstream cluster using the <code class="literal">nmc</code> <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">tool</a>.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</pre></div><p>In the <code class="literal">network</code> folder, the <code class="literal">configure-network.sh</code> file contains the networking configuration for the downstream cluster.
The following script tries to statically configure a NIC in the case the bare metal object has been created with a secret containing the static network configuration covered in the Additional Features with ZTP - advanced network configuration (<a class="xref" href="atip.html#ztp-add-advanced-network" title="21.5.5. Additional Features with ZTP - advanced network configuration">Section 21.5.5, “Additional Features with ZTP - advanced network configuration”</a>) section.
Also, it uses the networking information to generate the network configuration for the downstream cluster using the <code class="literal">nmc</code> <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">tool</a>.</p><div class="itemizedlist" id="ztp-add-custom-script-growfs"><ul class="itemizedlist"><li class="listitem"><p>There is a custom script (<code class="literal">custom/scripts/growfs.sh</code>) which is required to grow the filesystem to the disk size once it’s installed during the process. The <code class="literal">growfs.sh</code> script contains the following information:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</pre></div><div id="id-1.7.3.9.3.24" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>You can add your own custom scripts to be executed during the provisioning process.
For more information please check the Edge Image Builder (<span class="intraxref">Book “SUSE Edge Documentation”</span>) documentation.</p></div><p><span class="strong"><strong>Image Creation</strong></span></p><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD/eib/:/eib \
 registry.opensuse.org/isv/suse/edge/edgeimagebuilder/containerfile/suse/edge-image-builder:1.0.0 \
 --config-file downstream-cluster-config.yaml --config-dir /eib --build-dir /eib/_build</pre></div><p>This will create the output ISO image file, named <code class="literal">eibimage-slemicro55rt-telco.raw</code>, based on the definition described above.</p></section><section class="sect2" id="ztp-single-node" data-id-title="Downstream Cluster Provisioning with ZTP (single-node)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.3 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (single-node)</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-single-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>.
Basically, this is the simplest way to automate the provisioning of a downstream cluster.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>) with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on <a class="link" href="atip-management-cluster.xml#metal3-media-server" target="_blank">this section</a>.</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the <a class="link" href="atip-management-cluster.xml" target="_blank">Management Cluster</a> section for more information.</p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automated-singlenode1.png"><img src="images/atip-automated-singlenode1.png" width="NaN" alt="atip automated singlenode1" title="atip automated singlenode1"/></a></div></div><p>There are two different steps to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the bare metal Host to make it available for the provisioning process.</p></li><li class="listitem"><p>Provision the bare metal Host to install and configure the operating system and the Kubernetes cluster.</p></li></ol></div><p><span class="strong"><strong>Enroll the bare metal Host</strong></span></p><p>The first step is to enroll the new bare metal host in the management cluster to make it available to be provisioned.
To do that, the following file (<code class="literal">bmh-example.yaml</code>) has to be created in the management-cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</pre></div><p>where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_USERNAME}</code> - The username for the <code class="literal">BMC</code> of the new bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_PASSWORD}</code> - The password for the <code class="literal">BMC</code> of the new bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_MAC}</code> - The <code class="literal">MAC</code> address of the new bare metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_ADDRESS}</code> - The <code class="literal">URL</code> for the bare metal host <code class="literal">BMC</code> (e.g <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). If you want to know more about the different options available depending on your hardware provider, please check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><p>Once the file is created, the following command has to be executed in the management cluster to start enrolling the new bare metal host in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example.yaml</pre></div><p>The new bare metal host object will be enrolled changing its state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh</pre></div><div id="id-1.7.3.9.4.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object will be in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object will change its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. You can check the information using the following command <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p id="ztp-single-node-provision"><span class="strong"><strong>Provision Step</strong></span></p><p>Once the bare metal host is enrolled and available, the next step is to provision the bare metal host to install and configure the operating system and the Kubernetes cluster.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) has to be created in the management-cluster with the following information (the <code class="literal">capi-provisioning-example.yaml</code> can be generated joining the following blocks).</p><div id="id-1.7.3.9.4.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></div><p>The following block is the cluster definition where the networking can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</pre></div><p>The <code class="literal">Metal3Cluster</code> object to specify the control plane endpoint (replacing the <code class="literal">${DOWNSTREAM_CONTROL_PLANE_IP}</code>) to be configured and the <code class="literal">noCloudProvider</code> because a bare metal node will be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object to specify the control plane configuration to be used and the <code class="literal">Metal3MachineTemplate</code> object to specify the control plane image to be used.
Also, it contains the information about the number of replicas to be used (in this case, one) and the <code class="literal">CNI</code> plugin to be used (in this case, <code class="literal">Cilium</code>).
The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with some information like a systemd named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.
The last block of information contains the Kubernetes version to be used. The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object to specify the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used doing a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used doing a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>) and the <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object to specify the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new bare metal host:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect2" id="ztp-multi-node" data-id-title="Downstream Cluster Provisioning with ZTP (multi-node)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.4 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (multi-node)</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-multi-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code> and <code class="literal">MetalLB</code> as a load balancer strategy.
Basically, this is the simplest way to automate the provisioning of a downstream cluster. The following diagram shows the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code> and <code class="literal">MetalLB</code>.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>) with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on <a class="link" href="atip-management-cluster.xml#metal3-media-server" target="_blank">this section</a>.</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the <a class="link" href="atip-management-cluster.xml" target="_blank">Management Cluster</a> section for more information.</p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automate-multinode1.png"><img src="images/atip-automate-multinode1.png" width="NaN" alt="atip automate multinode1" title="atip automate multinode1"/></a></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the  three bare metal Hosts to make them available for the provisioning process.</p></li><li class="listitem"><p>Provision the three bare metal Host to install and configure the operating system and the Kubernetes cluster using <code class="literal">MetalLB</code>.</p></li></ol></div><p><span class="strong"><strong>Enroll the bare metal Hosts</strong></span></p><p>The first step is to enroll the three bar metal hosts in the management cluster to make them available to be provisioned.
To do that, the following files (<code class="literal">bmh-example-node1.yaml</code>, <code class="literal">bmh-example-node2.yaml</code> and <code class="literal">bmh-example-node3.yaml</code>) have to be created in the management-cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled in the management cluster.</p><div id="id-1.7.3.9.5.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></li><li class="listitem"><p>Only will be explained 1 host to make the process clear, but the same process has to be done for the other two nodes.</p></li></ul></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</pre></div><p>where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_NODE1_USERNAME}</code> - The username for the BMC of the first bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_PASSWORD}</code> - The password for the BMC of the first bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_MAC}</code> - The MAC address of the first bare metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_ADDRESS}</code> - The URL for the first bare metal host BMC (e.g <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). If you want to know more about the different options available depending on your hardware provider, please check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><p>Once the file is created, the following command has to be executed in the management cluster to start enrolling the bare metal hosts in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</pre></div><p>The new bare metal host objects will be enrolled changing their state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh -o wide</pre></div><div id="id-1.7.3.9.5.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object will be in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object will change its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. You can check the information using the following command <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p><span class="strong"><strong>Provision Step</strong></span></p><p>Once the three bar metal hosts are enrolled and available, the next step is to provision the bare metal hosts to install and configure the operating system and the Kubernetes cluster creating a load balancer to manage them.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) has to be created in the management-cluster with the following information (the `capi-provisioning-example.yaml can be generated joining the following blocks).</p><div id="id-1.7.3.9.5.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></li><li class="listitem"><p>The <code class="literal">VIP</code> address is a reserved IP address that is not assigned to any node and is used to configure the load balancer.</p></li></ul></div></div><p>The cluster definition where the cluster network can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</pre></div><p>The <code class="literal">Metal3Cluster</code> object to specify the control plane endpoint which uses the <code class="literal">VIP</code> address already reserved (replacing the <code class="literal">${DOWNSTREAM_VIP_ADDRESS}</code>) to be configured and the <code class="literal">noCloudProvider</code> because the three bar metal nodes will be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object to specify the control plane configuration to be used and the <code class="literal">Metal3MachineTemplate</code> object to specify the control plane image to be used.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The number of replicas to be used (in this case, 3)</p></li><li class="listitem"><p>The advertisement mode to be used by the Load Balancer (<code class="literal">address</code> will use the L2 implementation), as well as the address to be used (replacing the <code class="literal">${EDGE_VIP_ADDRESS}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">serverConfig</code> with the <code class="literal">CNI</code> plugin to be used (in this case, <code class="literal">Cilium</code>), and the <code class="literal">tlsSan</code> to be used to configure the <code class="literal">VIP</code> address.</p></li><li class="listitem"><p>The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with some information like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The systemd service named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p>The systemd files to install the <code class="literal">MetalLB</code> operator and the <code class="literal">endpoint-copier-operator</code> operator.</p></li><li class="listitem"><p>The <code class="literal">storage</code> block which contains the helm charts to be used to install the <code class="literal">MetalLB</code> and the <code class="literal">endpoint-copier-operator</code>.</p></li><li class="listitem"><p>The <code class="literal">metalLB</code> custom resource file with the <code class="literal">IPaddressPool</code> and the <code class="literal">L2Advertisement</code> to be used (replacing the <code class="literal">${EDGE_VIP_ADDRESS}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">endpoint-svc.yaml</code> file to be used to configure the <code class="literal">kubernetes-vip</code> service to be used by the <code class="literal">MetalLB</code> to manage the <code class="literal">VIP</code> address.</p></li></ul></div></li><li class="listitem"><p>The last block of information contains the Kubernetes version to be used. The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  registrationMethod: "address"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: metallb-endpointcopier-operators.service
              enabled: true
              contents: |
                [Unit]
                Description=metallb-endpointcopier-operators
                Wants=network-online.target
                After=rke2-server.service
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "while [ ! -d /var/lib/rancher/rke2/server/manifests ]; do sleep 1; done"
                ExecStart=/bin/sh -c "cp /var/metallb-endpointcopier-operators.yaml /var/lib/rancher/rke2/server/manifests/metallb-endpointcopier-operators.yaml"
                [Install]
                WantedBy=multi-user.target
            - name: metallb-cr.service
              enabled: true
              contents: |
                [Unit]
                Description=metallb-cr
                Wants=network-online.target
                After=metallb-endpointcopier-operators.service
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "while [ ! -f /var/lib/rancher/rke2/bin/kubectl ] &amp;&amp; [ ! -f /etc/rancher/rke2/rke2.yaml ]; do sleep 1; done"
                ExecStart=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get deployment --ignore-not-found metallb-controller | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStart=/bin/sh -c "/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml rollout status deployment metallb-controller -n default --timeout=150s"
                ExecStart=/bin/sh -c "cp /var/metallb-cr.yaml /var/lib/rancher/rke2/server/manifests/"
                ExecStartPost=/bin/sh -c "cp /var/endpoint-svc.yaml /var/lib/rancher/rke2/server/manifests/"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /var/metallb-endpointcopier-operators.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: default
                  spec:
                    repo: https://suse-edge.github.io/metallb
                    chart: metallb
                    targetNamespace: default
                    version: 0.14.3
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: default
                  spec:
                    repo: https://suse-edge.github.io/endpoint-copier-operator
                    chart: endpoint-copier-operator
                    targetNamespace: default
                    version: 0.2.0
            - path: /var/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: default
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: default
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                  spec:
                    internalTrafficPolicy: Cluster
                    ipFamilies:
                    - IPv4
                    ipFamilyPolicy: SingleStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    sessionAffinity: None
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "Node-multinode-cluster"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object to specify the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used doing a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used doing a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>) and the <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object to specify the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new three bar metal hosts:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect2" id="ztp-add-advanced-network" data-id-title="Additional Features with ZTP - advanced network configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.5 </span><span class="title-name">Additional Features with ZTP - advanced network configuration</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-add-advanced-network">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provisioning of the downstream clusters using advanced network configuration like DHCP-less, bond+vlans. The following sections describes the differences that can be used to automate the provisioning of the downstream clusters using advanced network configuration.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the network folder and the script following this section (<a class="xref" href="atip.html#ztp-add-network-eib">???TITLE???</a>)</p></li><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>) has to be located in the management cluster exactly on the path you configured on <a class="link" href="atip-management-cluster.xml#metal3-media-server" target="_blank">this section</a>.</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the <a class="link" href="atip-management-cluster.xml" target="_blank">Management Cluster</a> section for more information.</p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Using the following two section as the base to enroll and provision the hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (single-node) (<a class="xref" href="atip.html#ztp-single-node" title="21.5.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 21.5.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>)</p></li><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (multi-node) (<a class="xref" href="atip.html#ztp-multi-node" title="21.5.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 21.5.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a>)</p></li></ul></div><p>The changes required to enable the advanced network configuration are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Enrollment step: The following new example file with a secret containing the information about the <code class="literal">networkData</code> to be used to configure for example, the static <code class="literal">IPs</code> and <code class="literal">VLAN</code> for the downstream cluster</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</pre></div><p>This file contains the <code class="literal">networkData</code> in a <code class="literal">nmstate</code> format to be used to configure the advance network configuration (e.g. <code class="literal">static IPs</code>, and <code class="literal">VLAN</code>) for the downstream cluster.
As you can see, the example shows the configuration to enable the interface with static IPs, as well as the configuration to enable the VLAN using the base interface.
Any other <code class="literal">nmstate</code> example could be defined to be used to configure the network for the downstream cluster in order to adapt to the specific requirements.
where the following variables have to be replaced with the real values:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${CONTROLPLANE1_INTERFACE}</code> - The control plane interface to be used for the edge cluster (e.g. <code class="literal">eth0</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_IP}</code> - The IP address to be used as a endpoint for the edge cluster (should match with the kubeapi-server endpoint).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_PREFIX}</code> - The CIDR to be used for the edge cluster (e.g. <code class="literal">24</code> in case you want <code class="literal">/24</code> or <code class="literal">255.255.255.0</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_GATEWAY}</code> - The gateway to be used for the edge cluster (e.g. <code class="literal">192.168.100.1</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_MAC}</code> - The MAC address to be used for the control plane interface (e.g. <code class="literal">00:0c:29:3e:3e:3e</code>).</p></li><li class="listitem"><p><code class="literal">${DNS_SERVER}</code> - The DNS to be used for the edge cluster (e.g. <code class="literal">192.168.100.2</code>).</p></li><li class="listitem"><p><code class="literal">${VLAN_ID}</code> - The VLAN ID to be used for the edge cluster (e.g. <code class="literal">100</code>).</p></li></ul></div><p>Also, the reference to that secret using <code class="literal">preprovisioningNetworkDataName</code> is needed in the <code class="literal">BaremetalHost</code> object at the end of the file to be enrolled in the management cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</pre></div><div id="id-1.7.3.9.6.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>In case you need to deploy a multi-node cluster, the same process has to be done for the other nodes.</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Provision step: The block of information related to the network-data has to be removed because the platform is including the network data configuration into the secret <code class="literal">controlplane-0-networkdata</code>.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><div id="id-1.7.3.9.6.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">Metal3DataTemplate</code> <code class="literal">networkData</code> and <code class="literal">Metal3 IPAM</code> is not currently supported, only the configuration via static secrets is fully supported.</p></div></section><section class="sect2" id="ztp-add-telco" data-id-title="Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.6 </span><span class="title-name">Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-add-telco">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the Telco features to be used in the downstream clusters in order to run Telco workloads on top of that servers.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the specific Telco packages following this section (<a class="xref" href="atip.html#ztp-add-telco-feature-eib">???TITLE???</a>)</p></li><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip.html#ztp-eib-edge-image" title="21.5.2. Generate the downstream Image using EIB">Section 21.5.2, “Generate the downstream Image using EIB”</a>)  has to be located in the management cluster exactly on the path you configured on <a class="link" href="atip-management-cluster.xml#metal3-media-server" target="_blank">this section</a>.</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the <a class="link" href="atip-management-cluster.xml" target="_blank">Management Cluster</a> section for more information.</p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Using the following two section as the base to enroll and provision the hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (single-node) (<a class="xref" href="atip.html#ztp-single-node" title="21.5.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 21.5.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>)</p></li><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (multi-node) (<a class="xref" href="atip.html#ztp-multi-node" title="21.5.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 21.5.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a>)</p></li></ul></div><p>The Telco features which are covered on this section are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>DPDK and VFs creation</p></li><li class="listitem"><p>SR-IOV and VFs allocation to be used by the workloads</p></li><li class="listitem"><p>CPU isolation and performance tuning</p></li><li class="listitem"><p>Huge-pages configuration</p></li><li class="listitem"><p>Kernel parameters tuning</p></li></ul></div><div id="id-1.7.3.9.7.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the Telco features, please check the following <a class="link" href="atip-features.xml" target="_blank">link</a></p></div><p>The changes required to enable the Telco features shown above are all inside the <code class="literal">RKE2ControlPlane</code> block in the provision file <code class="literal">capi-provisioning-example.yaml</code>. The rest of the information inside the file <code class="literal">capi-provisioning-example.yaml</code> are the same as the information covered on the provisioning section (<a class="xref" href="atip.html#ztp-single-node-provision">Section 21.5.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>).</p><p>To make the process clear, the changes required on that block (<code class="literal">RKE2ControlPlane</code>) to enable the Telco features are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">preRKE2Commands</code> to be used to execute the commands before the <code class="literal">RKE2</code> installation process. In this case, the <code class="literal">modprobe</code> command to enable the <code class="literal">vfio-pci</code> and the <code class="literal">SR-IOV</code> kernel modules.</p></li><li class="listitem"><p>The ignition file <code class="literal">/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</code> to be used in order to define the interfaces, driver, and number of <code class="literal">VFs</code> to be created and exposed to the workloads.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The values inside the configmap <code class="literal">sriov-custom-auto-config</code> are the only values to be replaced with the real values.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${RESOURCE_NAME1}</code> - The resource name to be used for the first <code class="literal">PF</code> interface (e.g. <code class="literal">sriov-resource-du1</code>). It will be added to the prefix <code class="literal">rancher.io</code> to be used as a label to be used by the workloads (e.g. <code class="literal">rancher.io/sriov-resource-du1</code>).</p></li><li class="listitem"><p><code class="literal">${SRIOV-NIC-NAME1}</code> - The name of the first <code class="literal">PF</code> interface to be used (e.g. <code class="literal">eth0</code>).</p></li><li class="listitem"><p><code class="literal">${PF_NAME1}</code> - The name of the first physical function <code class="literal">PF</code> to be used. You can generate more complex filters using this (e.g. <code class="literal">eth0#2-5</code>).</p></li><li class="listitem"><p><code class="literal">${DRIVER_NAME1}</code> - The driver name to be used for the first <code class="literal">VF</code> interface (e.g. <code class="literal">vfio-pci</code>).</p></li><li class="listitem"><p><code class="literal">${NUM_VFS1}</code> - The number of <code class="literal">VFs</code> to be created for the first <code class="literal">PF</code> interface (e.g. <code class="literal">8</code>).</p></li></ul></div></li></ul></div></li><li class="listitem"><p>The <code class="literal">/var/sriov-auto-filler.sh</code> to be used as a translator between the high-level configmap <code class="literal">sriov-custom-auto-config</code> and the <code class="literal">sriovnetworknodepolicy</code> which contains the low-level hardware information. This script has been created to abstract the user from the complexity to know in advance the hardware information. No changes are required in this file, but it should be present if we need to enable <code class="literal">sr-iov</code> and create <code class="literal">VFs</code>.</p></li><li class="listitem"><p>The kernel arguments to be used to enable the following features:</p></li></ul></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Parameter</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Value</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Description</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-30 and 33-62</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the timer tick on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>kthread_cpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the kthreads on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the interrupts on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>processor.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Prevents the CPU from dropping into a sleep state when idle</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>intel_idle.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Disables the intel_idle driver and allows acpi_idle to be used</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to use vfio for the dpdk interfaces</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Enables to use vfio for VFs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to set the size of huge pages to 1G</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Number of hugepages defined before</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; "><p>Default value to enable huge pages</p></td></tr></tbody></table></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The following systemd services in order to enable:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p>The <code class="literal">cpu-performance.service</code> to enable the CPU performance tuning. The <code class="literal">${CPU_FREQUENCY}</code> has to be replaced with the real values (e.g. <code class="literal">2500000</code> to set the CPU frequency to <code class="literal">2.5GHz</code>).</p></li><li class="listitem"><p>The <code class="literal">cpu-partitioning.service</code> to enable the isolation cores of <code class="literal">CPU</code> (e.g. <code class="literal">1-30,33-62</code>).</p></li><li class="listitem"><p>The <code class="literal">sriov-custom-auto-vfs.service</code> to install the <code class="literal">sriov</code> helm chart, wait until custom resources are created and run the <code class="literal">/var/sriov-auto-filler.sh</code> to replace the values in the config map <code class="literal">sriov-custom-auto-config</code> and create the <code class="literal">sriovnetworknodepolicy</code> to be used by the workloads.</p></li></ul></div></li><li class="listitem"><p>The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p></li></ul></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: kube-system
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n kube-system -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-performance.service
              enabled: true
              contents: |
                [Unit]
                Description=CPU perfomance
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "cpupower frequency-set -g performance; cpupower frequency-set -u ${CPU_FREQUENCY}; cpupower frequency-set -d ${CPU_FREQUENCY}"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SR-IOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
                ExecStartPost=/bin/sh -c "helm repo add suse-edge https://suse-edge.github.io/charts"
                ExecStartPost=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "helm --kubeconfig /etc/rancher/rke2/rke2.yaml install sriov-crd suse-edge/sriov-crd -n kube-system"
                ExecStartPost=/bin/sh -c "helm --kubeconfig /etc/rancher/rke2/rke2.yaml install sriov-network-operator suse-edge/sriov-network-operator -n kube-system"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new downstream cluster using the Telco features:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect2" id="ztp-private-registry" data-id-title="Additional Features with ZTP - Private Registry"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.7 </span><span class="title-name">Additional Features with ZTP - Private Registry</span></span> <a title="Permalink" class="permalink" href="atip.html#ztp-private-registry">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provision of the downstream clusters and configure the private registry as a mirror to enable the images to be used by the workloads.</p><p>The first step to enable the private registry is to create the secret containing the information about the private registry to be used by the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</pre></div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code>, and <code class="literal">ca.crt</code> are the certificates to be used to authenticate the private registry. The <code class="literal">username</code> and <code class="literal">password</code> are the credentials to be used to authenticate the private registry.</p><div id="id-1.7.3.9.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code>, <code class="literal">ca.crt</code> , <code class="literal">username</code> and <code class="literal">password</code> have to be encoded in base64 format before to be used in the secret.</p></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>Where the <code class="literal">registry.example.com</code> is the example name of the private registry to be used by the downstream cluster, and it should be replaced with the real values.</p></section></section><section class="sect1" id="id-lifecycle-actions" data-id-title="Lifecycle Actions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.6 </span><span class="title-name">Lifecycle Actions</span></span> <a title="Permalink" class="permalink" href="atip.html#id-lifecycle-actions">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section covers the lifecycle management actions of deployed ATIP clusters</p><section class="sect2" id="id-management-cluster-upgrades" data-id-title="Management cluster upgrades"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.6.1 </span><span class="title-name">Management cluster upgrades</span></span> <a title="Permalink" class="permalink" href="atip.html#id-management-cluster-upgrades">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Upgrading the management cluster there are different components that need to be upgraded. The following sections will cover the upgrade process for each of the components.</p><p><span class="strong"><strong>Update the Operating System</strong></span></p><p>SUSE releases new version of <code class="literal">SLE Micro</code> at regular intervals. To make it easy for customers to migrate to a new minor version and minimize downtime, SUSE supports migrating online while the system is running.
SLE Micro uses transactional updates to upgrade from one version to the next. This has the following advantages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The system is always in a defined state until the first RPM is updated.</p></li><li class="listitem"><p>Canceling is possible until the first RPM is updated.</p></li><li class="listitem"><p>Simple recovery if there is an error.</p></li><li class="listitem"><p>It is possible to do a “rollback” via system tools—no backup or restore needed.</p></li><li class="listitem"><p>Use of all active repositories.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">transactional-update</pre></div><p>In case you want to revert the migration, you can use the following command:</p><div class="verbatim-wrap"><pre class="screen">transactional-update rollback last</pre></div><div id="id-1.7.3.10.3.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the update process, please check <a class="link" href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/sec-transactional-udate.html#sec-command-list" target="_blank">Transactional Update</a></p></div><p><span class="strong"><strong>Upgrade the RKE2 Cluster</strong></span></p><p>To upgrade <code class="literal">RKE2</code> from an older version you can re-run the installation script using the new version, for example:</p><div class="verbatim-wrap"><pre class="screen">curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=vX.Y.Z+rke2rN sh -</pre></div><p>After that, just restart the <code class="literal">rke2-server</code> service:</p><div class="verbatim-wrap"><pre class="screen"># server nodes
systemctl restart rke2-server</pre></div><p>or the agent service:</p><div class="verbatim-wrap"><pre class="screen"># agent nodes
systemctl restart rke2-agent</pre></div><div id="id-1.7.3.10.3.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the upgrade process, please refer to the <a class="link" href="https://docs.rke2.io/upgrade" target="_blank">RKE2 documentation</a></p></div><p><span class="strong"><strong>Upgrade the Rancher Prime</strong></span></p><p>To upgrade the <code class="literal">Rancher Prime</code> you can use the following command to update the helm repository cache, and fetch the latest chart to install Rancher from the helm chart repository:</p><div class="verbatim-wrap"><pre class="screen">helm repo update
helm fetch rancher-prime/rancher</pre></div><p>After that, the easy way to upgrade is to export your current configurations to a file, and then upgrade the Rancher Prime version using that previous file.
In case any change is required in the new version, the file can be edited before the upgrade.</p><div class="verbatim-wrap"><pre class="screen">helm get values rancher -n cattle-system -o yaml &gt; rancher-values.yaml
helm upgrade rancher rancher-prime/rancher \
  --namespace cattle-system \
  -f values.yaml \
  --version=2.x.y</pre></div><div id="id-1.7.3.10.3.23" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the upgrade process, please refer to the <a class="link" href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades" target="_blank">Upgrade Rancher Prime</a> documentation</p></div><p><span class="strong"><strong>Upgrade Metal<sup>3</sup></strong></span></p><p>To upgrade the <code class="literal">Metal<sup>3</sup></code> you can use the following command to update the helm repository cache, and fetch the latest chart to install <code class="literal">Metal<sup>3</sup></code> from the helm chart repository:</p><div class="verbatim-wrap"><pre class="screen">helm repo update
helm fetch suse-edge/metal3</pre></div><p>After that, the easy way to upgrade is to export your current configurations to a file, and then upgrade the <code class="literal">Metal<sup>3</sup></code> version using that previous file.
In case any change is required in the new version, the file can be edited before the upgrade.</p><div class="verbatim-wrap"><pre class="screen">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.0</pre></div></section><section class="sect2" id="id-downstream-cluster-upgrades" data-id-title="Downstream cluster upgrades"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.6.2 </span><span class="title-name">Downstream cluster upgrades</span></span> <a title="Permalink" class="permalink" href="atip.html#id-downstream-cluster-upgrades">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Upgrading the downstream clusters there are different components that need to be upgraded. The following sections will cover the upgrade process for each of the components.</p><p><span class="strong"><strong>Upgrade the Operating System</strong></span></p><p>For this process, check the following <a class="link" href="atip-automated-provision.xml#ztp-eib-edge-image" target="_blank">reference</a> to build the new image with a new operating system version.
With this new image generated by <code class="literal">EIB</code>, the next provision phase will use the new operating version provide.
In the following step, the new image will be used to upgrade the nodes.</p><p><span class="strong"><strong>Upgrade the RKE2 Cluster</strong></span></p><p>The changes required to upgrade the <code class="literal">RKE2</code> cluster using the automated workflow are the folloing:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Change the block <code class="literal">RKE2ControlPlane</code> in the <code class="literal">capi-provisioning-example.yaml</code> showed in the following <a class="link" href="atip-automated-provision.xml#ztp-single-node-provision" target="_blank">section</a>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Add the rollout strategy in the spec.</p></li><li class="listitem"><p>Change the version of the <code class="literal">RKE2</code> cluster to the new version replacing <code class="literal">${RKE2_NEW_VERSION}</code>.</p></li></ul></div></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_NEW_VERSION}
    nodeName: "localhost.localdomain"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Change the block <code class="literal">Metal3MachineTemplate</code> in the <code class="literal">capi-provisioning-example.yaml</code> showed in the following <a class="link" href="atip-automated-provision.xml#ztp-single-node-provision" target="_blank">section</a>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Change the image name and checksum to the new version generated on the previous step.</p></li><li class="listitem"><p>Add the directive <code class="literal">nodeReuse</code> to <code class="literal">true</code> to avoid the creation of a new node.</p></li><li class="listitem"><p>Add the directive <code class="literal">automatedCleaningMode</code> to <code class="literal">metadata</code> in order to enable the automated cleaning for the node.</p></li></ul></div></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</pre></div><p>After making these changes, the <code class="literal">capi-provisioning-example.yaml</code> file can be applied to the cluster using the following command:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f capi-provisioning-example.yaml</pre></div></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="id-product-documentation.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part V </span>Product Documentation</span></a> </div><div><a class="pagination-link next" href="id-appendix.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Appendix A </span>Appendix</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip.html#id-concept-architecture"><span class="title-number">21.1 </span><span class="title-name">Concept &amp; Architecture</span></a></span></li><li><span class="section"><a href="atip.html#id-requirements-assumptions"><span class="title-number">21.2 </span><span class="title-name">Requirements &amp; Assumptions</span></a></span></li><li><span class="section"><a href="atip.html#id-setting-up-the-management-cluster"><span class="title-number">21.3 </span><span class="title-name">Setting up the Management Cluster</span></a></span></li><li><span class="section"><a href="atip.html#id-telco-features-configuration"><span class="title-number">21.4 </span><span class="title-name">Telco Features Configuration</span></a></span></li><li><span class="section"><a href="atip.html#id-automated-provisioning-with-ztp"><span class="title-number">21.5 </span><span class="title-name">Automated Provisioning with ZTP</span></a></span></li><li><span class="section"><a href="atip.html#id-lifecycle-actions"><span class="title-number">21.6 </span><span class="title-name">Lifecycle Actions</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>